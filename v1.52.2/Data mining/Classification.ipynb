{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[Questionnaire only](#Questionnaire-only)\n",
    "  - [Can the answers to the scientific questions be used to predict if the questionnaire was filled before or after the game?](#Can-the-answers-to-the-scientific-questions-be-used-to-predict-if-the-questionnaire-was-filled-before-or-after-the-game?)\n",
    "    - [If scientific questions are coded by answers](#If-scientific-questions-are-coded-by-answers)\n",
    "    - [If scientific questions are coded by correctedness](#If-scientific-questions-are-coded-by-correctedness)\n",
    "[RedMetrics only](#RedMetrics-only)\n",
    "  - [Can the score of a player be predicted with their RedMetrics data?](#Can-the-score-of-a-player-be-predicted-with-their-RedMetrics-data?)\n",
    "\n",
    "[Questionnaire and RedMetrics](#Questionnaire-and-RedMetrics)\n",
    "  - [Can the biology level of a player be predicted using the game data?](#Can-the-biology-level-of-a-player-be-predicted-using-the-game-data?)\n",
    "  - [Can the gaming profile of a player be predicted using the game data?](#Can-the-gaming-profile-of-a-player-be-predicted-using-the-game-data?)\n",
    "  - [Can the completion time of each chapter be used to predict if a player is going to answer a specific scientific question correctly?](#Can-the-completion-time-of-each-chapter-be-used-to-predict-if-a-player-is-going-to-answer-a-specific-scientific-question-correctly?)\n",
    "  - [Can the game data be used to predict the performance on a sub-group of scientific questions?](#Can-the-game-data-be-used-to-predict-the-performance-on-a-sub-group-of-scientific-questions?)\n",
    "    - [Using an arbitrary classification of questions](#Using-an-arbitrary-classification-of-questions)\n",
    "      - [Hard questions](#Hard-questions)\n",
    "      - [Biobrick symbol recognition](#Biobrick-symbol-recognition)\n",
    "      - [Easy questions](#Easy-questions)\n",
    "    - [Using Bloom's taxonomy](#Using-Bloom's-taxonomy)\n",
    "      - [knowledge questions](#knowledge-questions)\n",
    "      - [comprehension questions](#comprehension-questions)\n",
    "      - [application questions](#application-questions)\n",
    "      - [analysis questions](#analysis-questions)\n",
    "      - [synthesis questions](#synthesis-questions)\n",
    "  - [Can the completion time be predicted from questionnaire answers?](#Can-the-completion-time-be-predicted-from-questionnaire-answers?)\n",
    "    - [From the before questionnaire](#From-the-before-questionnaire)\n",
    "    - [From the after questionnaire](#From-the-after-questionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run dataFormating.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print (sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "from math import *\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import normaltest\n",
    "\n",
    "from matplotlib.pyplot import boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the answers to the scientific questions be used to predict if the questionnaire was filled before or after the game?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I am using only decision tree methods here because other methods like naive bayes do not make sense on categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If scientific questions are coded by answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that correspond to scientific questions\n",
    "scientificColumns = [x for x in list(defForms.columns.values) if x[0] == \"Q\"]\n",
    "\n",
    "# Pick features and target\n",
    "features = defForms.loc[:, scientificColumns]\n",
    "target = defForms[\"temporality\"].astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using decision trees -accounts for the small size of the dataset and the categorical nature of the features\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0, max_features=\"auto\")\n",
    "scores = cross_val_score(clf, features, target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using random forests -accounts for the small size of the dataset and the categorical nature of the features, limit overfitting\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "scores = cross_val_score(clf, features, target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using extra tree classifiers, more random than random forest methods\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "scores = cross_val_score(clf, features, target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Accuracy is around 85%. Not bad but we expected better (17/01/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If scientific questions are coded by correctedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that correspond to scientific questions\n",
    "scientificColumns = [x for x in list(defCorrectedForms.columns.values) if x[0] == \"Q\"]\n",
    "\n",
    "# Pick features and target\n",
    "features = defCorrectedForms.loc[:, scientificColumns]\n",
    "target = defCorrectedForms[\"temporality\"].astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using decision trees -accounts for the small size of the dataset and the categorical nature of the features\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0, max_features=\"auto\")\n",
    "scores = cross_val_score(clf, features, target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using random forests -accounts for the small size of the dataset and the categorical nature of the features, limit overfitting\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "scores = cross_val_score(clf, features, target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using extra tree classifiers, more random than random forest methods\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "scores = cross_val_score(clf, features, target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Accuracy is around 80%. Not bad but we expected better (19/12/2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedMetrics only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RedMetrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosttestUserIds(gfdf):\n",
    "    return gfdf[gfdf[QTemporality] == answerTemporalities[1]][QUserId].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataWebgl1522 = prepareAllData(getAllUserVectorData(\n",
    "    getPosttestUserIds(gfdfWebgl1522UniqueProfiles),\n",
    "    rmdfWebgl1522UniqueProfiles,\n",
    "    gfdfWebgl1522UniqueProfiles,\n",
    "    _source = correctAnswers + demographicAnswers,\n",
    "    _printDebug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataWebgl1522Volunteers = prepareAllData(getAllUserVectorData(\n",
    "    getAllResponders(gfdfWebgl1522PretestPosttestUniqueProfilesVolunteers),\n",
    "    rmdfWebgl1522PretestPosttestUniqueProfilesVolunteers,\n",
    "    gfdfWebgl1522PretestPosttestUniqueProfilesVolunteers,\n",
    "    _source = correctAnswers + demographicAnswers,\n",
    "    _printDebug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataWebgl160 = prepareAllData(getAllUserVectorData(\n",
    "    getPosttestUserIds(gfdfWebgl160UniqueProfiles),\n",
    "    rmdfWebgl160UniqueProfiles,\n",
    "    gfdfWebgl160UniqueProfiles,\n",
    "    _source = correctAnswers + demographicAnswers,\n",
    "    _printDebug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataWebgl160Volunteers = prepareAllData(getAllUserVectorData(\n",
    "    getAllResponders(gfdfWebgl160PretestPosttestUniqueProfilesVolunteers),\n",
    "    rmdfWebgl160PretestPosttestUniqueProfilesVolunteers,\n",
    "    gfdfWebgl160PretestPosttestUniqueProfilesVolunteers,\n",
    "    _source = correctAnswers + demographicAnswers,\n",
    "    _printDebug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataPlaytestPhase2 = prepareAllData(getAllUserVectorData(\n",
    "    getAllResponders(gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase2),\n",
    "    rmdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase2,\n",
    "    gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase2,\n",
    "    _source = correctAnswers + demographicAnswers,\n",
    "    _printDebug=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the score of a player be predicted with their RedMetrics data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnonymousData(allDataClassif):\n",
    "    return allDataClassif.drop(\"anonymousID\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to exclude: contain direct information on posttest score\n",
    "dropPosttestColumns = allDataClassif.columns & (deltaQuestions + posttestQuestions + [\"scoreposttest\", \"scoredelta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnscaledFeatures(anonymousData):\n",
    "    # Only select rows where scoreafter is not negative\n",
    "    return anonymousData[anonymousData[\"scoreposttest\"] >= 0].drop(dropPosttestColumns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesTarget(allDataClassif, chosenModel = Lasso):\n",
    "    # Remove id\n",
    "    anonymousData = getAnonymousData(allDataClassif)\n",
    "\n",
    "    # Get features and target\n",
    "    # Only select rows where scoreafter is not negative\n",
    "    unscaledFeatures = getUnscaledFeatures(anonymousData)\n",
    "    target = anonymousData[anonymousData[\"scoreposttest\"] >= 0][\"scoreposttest\"]\n",
    "\n",
    "    # Center and scale data\n",
    "    #features = preprocessing.scale(unscaledFeatures)\n",
    "\n",
    "    # Center and scale data variant\n",
    "    standardScaler = preprocessing.StandardScaler()\n",
    "    standardScaler.fit(unscaledFeatures)\n",
    "    features = standardScaler.transform(unscaledFeatures)\n",
    "    \n",
    "    # Run Lasso regression with cross-validation\n",
    "    model = chosenModel()\n",
    "    scores = cross_val_score(model, features, target, cv=10)\n",
    "    boxplot(scores)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    model.fit(features, target)    \n",
    "    \n",
    "    return scores, standardScaler, model, features, target, unscaledFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, standardScaler, model, features, target, unscaledFeatures = getFeaturesTarget(allDataClassif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvertedCriteria(allData, criteria):\n",
    "    result = allData.copy()\n",
    "    \n",
    "    if not (len(result.columns & criteria) == len(criteria)):\n",
    "        print(\"not all criteria are in input columns\")\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        result[criterion] = 1 / (1 + result[criterion])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataClassifInv = getAllDataClassif(getInvertedCriteria(allData, totalTimesCriteria + completionTimesCriteria))\n",
    "scoresInv, standardScalerInv, modelInv, featuresInv, targetInv, unscaledFeaturesInv = getFeaturesTarget(allDataClassifInv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of a single score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(standardScaler, model, unscaledX):\n",
    "    X = standardScaler.transform([unscaledX])\n",
    "    return model.predict(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictionVsActual(standardScaler, model, allDataClassif):\n",
    "    unscaledFeatures = getUnscaledFeatures(getAnonymousData(allDataClassif))\n",
    "    \n",
    "    result = pd.DataFrame(index = unscaledFeatures.index, columns=[\"predicted\", \"actual\", \"error\"], data = -1)\n",
    "\n",
    "    for userId in unscaledFeatures.index:\n",
    "        unscaledX = unscaledFeatures.loc[userId].values\n",
    "        actualScore = allDataClassif.loc[userId, \"scoreposttest\"]\n",
    "\n",
    "        result.loc[userId, \"predicted\"] = getPrediction(standardScaler, model, unscaledX)\n",
    "        result.loc[userId, \"actual\"] = actualScore\n",
    "        result.loc[userId, \"error\"] = result.loc[userId, \"predicted\"] - result.loc[userId, \"actual\"]\n",
    "        \n",
    "    r2Coef = model.score(standardScaler.transform(unscaledFeatures), result[\"actual\"].values)\n",
    "        \n",
    "    return result, r2Coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use allData from online campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [allDataWebgl1522,\n",
    "           allDataWebgl1522Volunteers,\n",
    "           allDataWebgl160,\n",
    "           allDataWebgl160Volunteers,\n",
    "           allDataPlaytestPhase2\n",
    "          ]\n",
    "\n",
    "for sample in samples:\n",
    "    _allDataClassif = getAllDataClassif(sample)\n",
    "    result, r2Coef = getPredictionVsActual(standardScaler, model, _allDataClassif)\n",
    "    print(\"{0:0=2d}\".format(len(_allDataClassif)) + \":     \" + str(r2Coef))\n",
    "    \n",
    "    _allDataClassifInv = getAllDataClassif(getInvertedCriteria(sample, totalTimesCriteria + completionTimesCriteria))\n",
    "    resultInv, r2CoefInv = getPredictionVsActual(standardScalerInv, modelInv, _allDataClassifInv)\n",
    "    print(\"{0:0=2d}\".format(len(_allDataClassifInv)) + \" inv: \" + str(r2CoefInv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the most important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLassoModelCoefficients(model, unscaledFeatures, useAbs = True):\n",
    "    nonNullIndices = np.nonzero(model.coef_)    \n",
    "    data = model.coef_[nonNullIndices]\n",
    "    if useAbs:\n",
    "        data = abs(data)    \n",
    "    lassoModelParameters = pd.Series(\n",
    "        index = unscaledFeatures.columns[nonNullIndices],\n",
    "        data = data\n",
    "    ).sort_values()\n",
    "    return lassoModelParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLassoModelCoefficients(model, unscaledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLassoModelCoefficients(modelInv, unscaledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unscaledFeatures = getUnscaledFeatures(getAnonymousData(allDataClassifWebgl160Volunteers))\n",
    "#unscaledX = unscaledFeatures.iloc[0].values\n",
    "#X = standardScaler.transform([unscaledX])\n",
    "#model.predict(X)[0]\n",
    "\n",
    "#X = (unscaledX - standardScaler.mean_) / standardScaler.scale_\n",
    "#model.predict([X])[0]\n",
    "\n",
    "#np.dot(model.coef_, X) + model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    anonymousData = getAnonymousData(allDataClassif)\n",
    "    \n",
    "    sortedUnscaledFeatures = anonymousData[anonymousData[\"scoreposttest\"] >= 0].sort_values(by=\"scoreposttest\").drop(dropPosttestColumns, axis = 1)\n",
    "    sortedTarget = sorted(anonymousData[anonymousData[\"scoreposttest\"] >= 0][\"scoreposttest\"])\n",
    "\n",
    "    # Center and scale data variant\n",
    "    sortedFeatures = standardScaler.transform(sortedUnscaledFeatures)\n",
    "\n",
    "    x = range(len(sortedFeatures))\n",
    "    alpha = 0.5\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title('Actual vs predicted score')\n",
    "    plt.xlabel('User index')\n",
    "    plt.ylabel('Score')\n",
    "    #plt.plot(x, model.predict(sortedFeatures), kind = 'bar')\n",
    "    #plt.plot(x, sortedTarget)\n",
    "    ax.bar(x, model.predict(sortedFeatures), alpha=alpha, label='predicted', linewidth=0)\n",
    "    ax.bar(x, sortedTarget,                  alpha=alpha, label='actual')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Score cannot be predicted by the table of RedMetrics data (19/07/2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesTargetSecondDegreePolynomial(allDataClassif, chosenModel = Lasso):\n",
    "    # Remove id\n",
    "    anonymousData = getAnonymousData(allDataClassif)\n",
    "\n",
    "    # Get features and target\n",
    "    # Only select rows where scoreafter is not negative\n",
    "    unscaledFeatures = getUnscaledFeatures(anonymousData)\n",
    "    target = anonymousData[anonymousData[\"scoreposttest\"] >= 0][\"scoreposttest\"]\n",
    "\n",
    "    # Add polynomial features\n",
    "    secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "    unscaledFeatures = secondDegreeFeatures.fit_transform(unscaledFeatures)\n",
    "\n",
    "    # Center and scale data variant\n",
    "    standardScaler = preprocessing.StandardScaler()\n",
    "    standardScaler.fit(unscaledFeatures)\n",
    "    features = standardScaler.transform(unscaledFeatures)\n",
    "    \n",
    "    # Run Lasso regression with cross-validation\n",
    "    model = chosenModel()\n",
    "    scores = cross_val_score(model, features, target, cv=10)\n",
    "    boxplot(scores)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    model.fit(features, target)\n",
    "    \n",
    "    return scores, standardScaler, model, features, target, unscaledFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeaturesTargetSecondDegreePolynomial(allDataClassif);\n",
    "getFeaturesTargetSecondDegreePolynomial(allDataClassifInv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Score cannot be predicted by the table of RedMetrics data + second degree polynomial (30/01/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try by reducing the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id\n",
    "anonymousData = getAnonymousData(allDataClassif)\n",
    "\n",
    "# Get features and target\n",
    "# Only select rows where scoreafter is not negative\n",
    "unscaledFeatures = anonymousData[anonymousData[\"scoreposttest\"] >= 0]\n",
    "\n",
    "#unscaledFeatures = unscaledFeatures[[\"craft\", \"death\", \"add\", \"remove\", \"reach\", \"maxChapter\"] + totalTimesCriteria + completionTimesCriteria]\n",
    "#unscaledFeatures = unscaledFeatures[[\"craft\", \"death\", \"add\", \"remove\", \"reach\", \"maxChapter\"]]\n",
    "#unscaledFeatures = unscaledFeatures[totalTimesCriteria]\n",
    "#unscaledFeatures = unscaledFeatures[completionTimesCriteria]\n",
    "#unscaledFeatures = unscaledFeatures[[\"maxChapter\", \"ch05completion\", \"ch07completion\", \"ch07total\", \"ch09total\"]]\n",
    "#unscaledFeatures = unscaledFeatures[['pretest Enjoyed playing', 'scorepretest', 'pretest Want to learn more about Biology', 'ch05total', 'ch07total']]\n",
    "#unscaledFeatures = unscaledFeatures[['ch05completion', 'ch08total', 'ch06total', 'scorepretest', 'pretest Want to learn more about Biology', 'ch05total', 'ch07total']]\n",
    "if 'columnsForRegression' in globals():\n",
    "    unscaledFeatures = unscaledFeatures[columnsForRegression]\n",
    "else:\n",
    "    unscaledFeatures = unscaledFeatures[['ch05completion', 'ch08total', 'ch06total', 'scorepretest', 'pretest Want to learn more about Biology', 'ch05total', 'ch07total']]\n",
    "\n",
    "target = anonymousData[anonymousData[\"scoreposttest\"] >= 0][\"scoreposttest\"]\n",
    "\n",
    "# Add polynomial features\n",
    "secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "features = secondDegreeFeatures.fit_transform(unscaledFeatures)\n",
    "\n",
    "# Center and scale data\n",
    "features = preprocessing.scale(unscaledFeatures)\n",
    "\n",
    "# Run Lasso regression with cross-validation\n",
    "model = Lasso()\n",
    "scores = cross_val_score(model, features, target, cv=10)\n",
    "boxplot(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "model.fit(features, target)\n",
    "\n",
    "getLassoModelCoefficients(model, unscaledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoresMean(allDataClassif, columnsSubset):\n",
    "    anonymousData = getAnonymousData(allDataClassif)\n",
    "    unscaledFeatures = anonymousData[anonymousData[\"scoreposttest\"] >= 0]\n",
    "    unscaledFeatures = unscaledFeatures[columnsSubset]\n",
    "    target = anonymousData[anonymousData[\"scoreposttest\"] >= 0][\"scoreposttest\"]\n",
    "    secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "    features = secondDegreeFeatures.fit_transform(unscaledFeatures)\n",
    "    features = preprocessing.scale(unscaledFeatures)\n",
    "    model = Lasso()\n",
    "    scores = cross_val_score(model, features, target, cv=10)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of possibles subsets of size n of a set of size 96\n",
    "import scipy.special\n",
    "scipy.special.binom(96, 3),\\\n",
    "scipy.special.binom(96, 4),\\\n",
    "scipy.special.binom(96, 5),\\\n",
    "scipy.special.binom(96, 6),\\\n",
    "scipy.special.binom(96, 7),\\\n",
    "scipy.special.binom(96, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getETA(computations, timestamp):\n",
    "    # computing speed: computations per second\n",
    "    computationSpeed = 2794155 / 42338\n",
    "    duration = computations / computationSpeed\n",
    "    eta = timestamp + pd.Timedelta(seconds = duration)\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import scipy.special\n",
    "import warnings\n",
    "from ipywidgets import Textarea, FloatText, ToggleButton, Checkbox\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "adc = allDataClassif.copy()\n",
    "#adc = allDataClassifInv.copy()\n",
    "\n",
    "predefinedCriteria = ['ch05completion', 'scorepretest', 'pretest Want to learn more about Biology', 'ch07total', 'ch05total',]\n",
    "\n",
    "criteria = list(\\\n",
    "    set(adc.columns)\\\n",
    "    - set(adc.columns & \\\n",
    "          (deltaQuestions + posttestQuestions \n",
    "           + [\"scoreposttest\", \"scoredelta\", 'scoreundefined', \"anonymousID\"]\n",
    "           + predefinedCriteria\n",
    "          ))\\\n",
    ")\n",
    "\n",
    "subsetSize = 4\n",
    "combinations = scipy.special.binom(len(criteria), subsetSize)\n",
    "print(\"#combinations=\"+str(combinations))\n",
    "print(\"ETA \" + str(getETA(combinations, pd.Timestamp.now())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "     # very long computation time: > 10h\n",
    "    maxScore = 0.37\n",
    "    i = 0\n",
    "    columnsForRegression = []\n",
    "    iterations = combinations+2\n",
    "\n",
    "    _progress = IntProgress(min=0, max=iterations)\n",
    "    _intText = IntText(0)\n",
    "    _currentBest = FloatText(0.0)\n",
    "    _currentCriteria = Textarea(\"\")\n",
    "    #_stopButton = ToggleButton(value=False, description='Stop')\n",
    "    #_stopCheckbox = Checkbox(value=False, description='Stop')\n",
    "\n",
    "    display(_progress)\n",
    "    display(_intText)\n",
    "    display(_currentBest)\n",
    "    display(_currentCriteria)\n",
    "    #display(_stopButton)\n",
    "    #display(_stopCheckbox)\n",
    "\n",
    "    iterator = itertools.combinations(criteria, subsetSize)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for columnsSubset in iterator:\n",
    "        #if _stopButton.value or _stopCheckbox.value or (i >= iterations):\n",
    "        if (i >= iterations):\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "            _progress.value += 1\n",
    "            _intText.value+= 1\n",
    "            score = getScoresMean(adc, list(columnsSubset) + predefinedCriteria)\n",
    "            if score > maxScore:\n",
    "                maxScore = score\n",
    "                _currentBest.value = score\n",
    "                columnsForRegression = list(columnsSubset) + predefinedCriteria\n",
    "                _currentCriteria.value = str(columnsForRegression)\n",
    "\n",
    "    print(\"--- executed %s / %s in %s seconds ---\" % (i, combinations, time.time() - start_time))\n",
    "    print(\"--- end time: \" + str(pd.Timestamp.now()))\n",
    "\n",
    "    maxScore, columnsForRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long to compute all\n",
    "(17 * 61124064 / 1000) / 3600,\\\n",
    "(249 * 57940519 / 15000) / 3600,\\\n",
    "(204 * 57940519 / 15000) / 3600,\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much computed in some duration\n",
    "durationSeconds = 5 * 60\n",
    "durationSeconds * 1000 / 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allDataClassif\n",
    "\n",
    "0.389994800369642, ['ch12completion', 'totalTime', 'pretest Studied biology']\n",
    "\n",
    "0.3899953439583282, ['scoreundefined', 'pretest Want to learn more about Biology', 'ch12total', 'ch10total', 'pretest Name: PR')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allDataClassifInv\n",
    "\n",
    "0.39870229095205095, ['pretest Want to learn more about Biology', 'ch01completion', 'reach', 'pretest Example: CDS', 'pretest Played Hero.Coli']\n",
    "\n",
    "['pretest Want to learn more about Biology', 'ch01completion', 'pretest Device: PCONS:RBS:FLHDC:TER', 'ch05total', 'scorepretest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Tried different combinations, but cannot find any interesting regression (02/02/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire and RedMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the biology level of a player be predicted using the game data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id\n",
    "anonymousData = gameAndCorrectedAfterDataClassif.drop(\"anonymousID\", axis = 1)\n",
    "\n",
    "# Get features and target\n",
    "# Only select rows where scoreafter is not negative\n",
    "features = anonymousData[anonymousData[\"scoreposttest\"] >= 0]\n",
    "features = features.loc[:,\"sessionsCount\":\"completionTime\"]\n",
    "target = anonymousData[anonymousData[\"scoreposttest\"] >= 0][\"biologyStudy\"]\n",
    "\n",
    "# Add polynomial features\n",
    "secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "features = secondDegreeFeatures.fit_transform(features)\n",
    "\n",
    "# Center and scale data\n",
    "features = preprocessing.scale(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Lasso regression with cross-validation\n",
    "model = Lasso()\n",
    "scores = cross_val_score(model, features, target, cv=10)\n",
    "boxplot(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion: No (30/01/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the gaming profile of a player be predicted using the game data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id\n",
    "anonymousData = gameAndCorrectedAfterDataClassif.drop(\"anonymousID\", axis = 1)\n",
    "\n",
    "# Get features and target\n",
    "# Only select rows where scoreafter is not negative\n",
    "features = anonymousData.loc[:,\"sessionsCount\":\"completionTime\"]\n",
    "target = sum(anonymousData[\"gameInterest\"], anonymousData[\"gameFrequency\"])\n",
    "\n",
    "# Add polynomial features\n",
    "secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "features = secondDegreeFeatures.fit_transform(features)\n",
    "\n",
    "# Center and scale data\n",
    "features = preprocessing.scale(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Lasso regression with cross-validation\n",
    "model = Lasso()\n",
    "scores = cross_val_score(model, features, target, cv=10)\n",
    "boxplot(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion: No (30/01/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Can the completion time of each chapter be used to predict if a player is going to answer a specific scientific question correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a question tag, plot scores of cross-validated model\n",
    "def tryClassification(data, scientificQuestion):\n",
    "    # Remove id\n",
    "    anonymousData = data.drop(\"anonymousID\", axis = 1)\n",
    "\n",
    "    # Get features and target\n",
    "    # Only select rows where scoreafter is not negative\n",
    "    features = anonymousData[anonymousData[\"scoreposttest\"] >= 0]\n",
    "    features = features.iloc[:,24:37]\n",
    "    target = anonymousData[anonymousData[\"scoreposttest\"] >= 0].loc[:,scientificQuestion].astype('int')\n",
    "\n",
    "    # Add polynomial features\n",
    "    secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "    features = secondDegreeFeatures.fit_transform(features)\n",
    "\n",
    "    # Center and scale data\n",
    "    features = preprocessing.scale(features)\n",
    "    \n",
    "    # Classify using extra tree classifiers, more random than random forest methods\n",
    "    clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "    scores = cross_val_score(clf, features, target, cv=5)\n",
    "    \n",
    "    # Display plot\n",
    "    fig, ax = plt.subplots()\n",
    "    boxplot(scores)\n",
    "    \n",
    "    return [scores.mean(), scores.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allScores = pd.DataFrame(index = [\"Mean\", \"Var\"])\n",
    "for question in [\"QGenotypePhenotype\", \"QBioBricksDevicesComposition\", \"QAmpicillin\", \"QBBNamePlasmid\", \"QBBFunctionTER\", \"QBBNamePromoter\", \"QBBFunctionGameCDS\", \"QBBNameTerminator\", \"QBBFunctionBiologyCDS\", \"QBBNameRBS\", \"QBBExampleCDS\", \"QBBNameCDS\", \"QBBFunctionPR\", \"QBBFunctionRBS\", \"QBBFunctionPlasmid\", \"QBBNameOperator\", \"QDeviceRbsPconsFlhdcTer\", \"QDevicePconsRbsFlhdcTer\", \"QDevicePbadRbsGfpTer\", \"QDevicePbadGfpRbsTer\", \"QDeviceGfpRbsPconsTer\", \"QDevicePconsGfpRbsTer\", \"QDeviceAmprRbsPconsTer\", \"QDeviceRbsPconsAmprTer\", \"QGreenFluorescence\", \"QUnequipDevice\", \"QDevicePbadRbsAraTer\"]:\n",
    "    questionTag = question\n",
    "    scores = tryClassification(gameAndCorrectedAfterDataClassif, questionTag)\n",
    "    allScores[questionTag] = scores\n",
    "allScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Redmetrics can be used to predict answers to certain scientific questions (29/05/2018)\n",
    "TODO Raphael: Check which questions you want additional analysis for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the game data be used to predict the performance on a sub-group of scientific questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoxplot(scores, title = ''):\n",
    "    # figure related code\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(scores)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([anonymousData.loc[:,\"sessionsCount\":\"completionTime\"], anonymousData.loc[:,\"gameInterest\":\"previousPlay\"]], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#anonymousData.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ingameCriteria = ['sessionsCount', 'scoreposttest', 'scoreundefined', 'complete',\n",
    "       'configure', 'craft', 'death', 'equip', 'unequip', 'add', 'remove',\n",
    "       'gotourl', 'pickup', 'reach', 'restart', 'selectmenu', 'start',\n",
    "       'scoredelta', 'maxChapter', 'efficiency', 'thoroughness', 'fun',\n",
    "       'completionTime', 'ch00completion', 'ch01completion',\n",
    "       'ch02completion', 'ch03completion', 'ch04completion',\n",
    "       'ch05completion', 'ch06completion', 'ch07completion',\n",
    "       'ch08completion', 'ch09completion', 'ch10completion',\n",
    "       'ch11completion', 'ch12completion', 'ch13completion',\n",
    "       'ch14completion', 'ch00total', 'ch01total', 'ch02total',\n",
    "       'ch03total', 'ch04total', 'ch05total', 'ch06total', 'ch07total',\n",
    "       'ch08total', 'ch09total', 'ch10total', 'ch11total', 'ch12total',\n",
    "       'ch13total', 'ch14total', 'totalTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot function\n",
    "#  questions: array of strings of question names\n",
    "def getPerformanceFromQuestionGroup(questions,\n",
    "                                    thresholdPercentage = 1.0,\n",
    "                                    extraTreesClassifier = False,\n",
    "                                    randomForestClassifier = False,\n",
    "                                    lasso = False,\n",
    "                                    histTarget = 0\n",
    "                                   ):\n",
    "    # Remove id\n",
    "    anonymousData = gameAndCorrectedAfterDataClassif.drop(\"anonymousID\", axis = 1)\n",
    "\n",
    "    # Get features and target\n",
    "    #features = pd.concat([anonymousData.loc[:,\"sessionsCount\":\"completionTime\"], anonymousData.loc[:,\"gameInterest\":\"previousPlay\"]], axis=1)\n",
    "    features = anonymousData.loc[:,ingameCriteria]\n",
    "    \n",
    "    digitalTarget = anonymousData.loc[:, questions].astype(int).sum(axis=1)\n",
    "    categoricalTarget = digitalTarget.apply(lambda x: 0 if x < thresholdPercentage*len(questions) else 1)\n",
    "\n",
    "    # Add polynomial features\n",
    "    secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "    features = secondDegreeFeatures.fit_transform(features)\n",
    "\n",
    "    # Center and scale data\n",
    "    features = preprocessing.scale(features)\n",
    "\n",
    "    if extraTreesClassifier:\n",
    "        # Classify using extra tree classifiers, more random than random forest methods\n",
    "        clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "        scores = cross_val_score(clf, features, categoricalTarget, cv=10)\n",
    "        print(\"ExtraTreesClassifier scores mean: \" + str(scores.mean()))\n",
    "\n",
    "        # Display plot\n",
    "        getBoxplot(scores, \"ExtraTreesClassifier boxplot\")\n",
    "        \n",
    "    if randomForestClassifier:\n",
    "        # Classify using random forests -accounts for the small size of the dataset and the categorical nature of the features, limit overfitting\n",
    "        clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "        scores = cross_val_score(clf, features, categoricalTarget)\n",
    "        print(\"RandomForestClassifier scores mean: \" + str(scores.mean()))\n",
    "\n",
    "        # Display plot\n",
    "        getBoxplot(scores, \"RandomForestClassifier boxplot\")\n",
    "        \n",
    "    if lasso:\n",
    "        # Run Lasso regression with cross-validation\n",
    "        model = Lasso()\n",
    "        scores = cross_val_score(model, features, digitalTarget, cv=10)\n",
    "        print(\"Lasso scores mean: \" + str(scores.mean()))\n",
    "\n",
    "        # Display plot\n",
    "        getBoxplot(scores, \"Lasso boxplot\")\n",
    "        \n",
    "    if histTarget > 0:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.hist(target, bins = range(histTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an arbitrary classification of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardQuestions = [\"QBBFunctionPR\", \"QBBNameOperator\", \"QDevicePbadRbsAraTer\"]\n",
    "getPerformanceFromQuestionGroup(hardQuestions, thresholdPercentage = 0.5, extraTreesClassifier = True, randomForestClassifier = True, lasso = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Very high quality prediction (29/05/18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biobrick symbol recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbSymbolRecognition = [\"QBBNamePlasmid\", \"QBBFunctionTER\", \"QBBNamePromoter\", \"QBBFunctionGameCDS\", \"QBBNameTerminator\", \"QBBFunctionBiologyCDS\", \"QBBNameRBS\", \"QBBExampleCDS\", \"QBBNameCDS\", \"QBBFunctionPR\", \"QBBFunctionRBS\", \"QBBFunctionPlasmid\", \"QBBNameOperator\"]\n",
    "getPerformanceFromQuestionGroup(bbSymbolRecognition, thresholdPercentage = 0.6, extraTreesClassifier = True, randomForestClassifier = True, lasso = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: No apparent possible prediction (1/02/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easyQuestions = [\"QBioBricksDevicesComposition\", \"QDeviceRbsPconsFlhdcTer\", \"QGreenFluorescence\"]\n",
    "getPerformanceFromQuestionGroup(easyQuestions, thresholdPercentage = 1.0, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Inconclusive (01/02/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bloom's taxonomy\n",
    "\n",
    "Not interpreted yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knowledge questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgeQuestions = [\"QAmpicillin\",\n",
    "                      \"QBBNamePlasmid\",\n",
    "                      \"QBBNamePromoter\",                      \n",
    "                      \"QBBNameTerminator\",\n",
    "                      \"QBBNameRBS\",\n",
    "                      \"QBBNameCDS\",\n",
    "                      \"QBBNameOperator\",\n",
    "                     ]\n",
    "getPerformanceFromQuestionGroup(knowledgeQuestions, thresholdPercentage = 0.7, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comprehension questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensionQuestions = [\"QBioBricksDevicesComposition\",\n",
    "                      \"QBBFunctionTER\",\n",
    "                      \"QBBFunctionPlasmid\",                      \n",
    "                      \"QUnequipDevice\",\n",
    "                     ]\n",
    "getPerformanceFromQuestionGroup(comprehensionQuestions, thresholdPercentage = 1.0, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### application questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationQuestions = [\"QGenotypePhenotype\",\n",
    "                      \"QBBExampleCDS\",\n",
    "                      \"QGreenFluorescence\",\n",
    "                     ]\n",
    "getPerformanceFromQuestionGroup(applicationQuestions, thresholdPercentage = 1.0, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisQuestions = [\"QBBFunctionGameCDS\",\n",
    "                      \"QBBFunctionBiologyCDS\",\n",
    "                      \"QBBFunctionPR\",\n",
    "                      \"QBBFunctionRBS\",\n",
    "                      \"QDevicePbadRbsAraTer\",\n",
    "                     ]\n",
    "getPerformanceFromQuestionGroup(analysisQuestions, thresholdPercentage = 0.7, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthesis questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesisQuestions = [\"QDeviceRbsPconsFlhdcTer\",\n",
    "                      \"QDevicePconsRbsFlhdcTer\",\n",
    "                      \"QDevicePbadRbsGfpTer\",                      \n",
    "                      \"QDevicePbadGfpRbsTer\",\n",
    "                      \"QDeviceGfpRbsPconsTer\",\n",
    "                      \"QDevicePconsGfpRbsTer\",\n",
    "                      \"QDeviceAmprRbsPconsTer\",\n",
    "                      \"QDeviceRbsPconsAmprTer\",\n",
    "                     ]\n",
    "getPerformanceFromQuestionGroup(synthesisQuestions, thresholdPercentage = 1.0, extraTreesClassifier = True, randomForestClassifier = True, lasso = True, histTarget = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the completion time be predicted from questionnaire answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the before questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove id\n",
    "anonymousData = gameAndCorrectedBeforeDataClassif.drop(\"anonymousID\", axis = 1)\n",
    "\n",
    "# Get features and target\n",
    "lastColumn = 'gender_Male'\n",
    "for potentialLastColumn in ['gender_Other', 'gender_Prefer not to say']:\n",
    "    if potentialLastColumn in anonymousData.columns:\n",
    "        lastColumn = potentialLastColumn\n",
    "features = anonymousData.loc[:,\"gameInterest\":lastColumn]\n",
    "target = anonymousData.loc[:,\"completionTime\"]\n",
    "\n",
    "# Add polynomial features\n",
    "secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "features = secondDegreeFeatures.fit_transform(features)\n",
    "\n",
    "# Center and scale data\n",
    "features = preprocessing.scale(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Lasso regression with cross-validation\n",
    "model = Lasso(max_iter=10000, alpha=10)\n",
    "scores = cross_val_score(model, features, target, cv=10)\n",
    "boxplot(scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try classification\n",
    "target = target.apply(lambda x: 0 if x < 7200 else 1) #0 if short, 1 if long\n",
    "\n",
    "# Classify using extra tree classifiers, more random than random forest methods\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "scores = cross_val_score(clf, features, target, cv=10)\n",
    "    \n",
    "# Display plot\n",
    "boxplot(scores)\n",
    "scores.mean()\n",
    "sum(target)/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: No (01/02/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the after questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove id\n",
    "anonymousData = gameAndCorrectedAfterDataClassif.drop(\"anonymousID\", axis = 1)\n",
    "\n",
    "# Get features and target\n",
    "lastColumn = 'gender_Male'\n",
    "for potentialLastColumn in ['gender_Other', 'gender_Prefer not to say']:\n",
    "    if potentialLastColumn in anonymousData.columns:\n",
    "        lastColumn = potentialLastColumn\n",
    "features = anonymousData.loc[:,\"gameInterest\":lastColumn]\n",
    "target = anonymousData.loc[:,\"completionTime\"]\n",
    "\n",
    "# Add polynomial features\n",
    "secondDegreeFeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "features = secondDegreeFeatures.fit_transform(features)\n",
    "\n",
    "# Center and scale data\n",
    "features = preprocessing.scale(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Lasso regression with cross-validation\n",
    "model = Lasso(max_iter=1000000)\n",
    "scores = cross_val_score(model, features, target, cv=10)\n",
    "boxplot(scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try classification\n",
    "target = target.apply(lambda x: 0 if x < 7200 else 1) #0 if short, 1 if long\n",
    "\n",
    "# Classify using extra tree classifiers, more random than random forest methods\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0, bootstrap=True)\n",
    "scores = cross_val_score(clf, features, target, cv=10)\n",
    "    \n",
    "# Display plot\n",
    "boxplot(scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Yes (29/05/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Yes but very unbalanced classes (29/05/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
