{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero.Coli Data Analysis Summary\n",
    "\n",
    "Interactive list of readworthy results from Hero.Coli data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Preparation](#Preparation)\n",
    "1. [Google form analysis](#gfdf)\n",
    "2. [Game sessions](#sessions)\n",
    "3. [Per session and per user analysis](#peruser)\n",
    "4. [User comparison](#usercomp)\n",
    "5. [Game map](#map)\n",
    "    1. [List of questions](#qlist)\n",
    "    2. [English](#enform)\n",
    "    3. [French](#frform)\n",
    "    4. [Language selection](#langsel)\n",
    "3. [Basic operations](#basicops)\n",
    "4. [Checkpoint / Question matching](#checkquestmatch)\n",
    "5. [Correlations between durations and score on questions](#Correlations-between-durations-and-score-on-questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run \"../Functions/8. RM-GF correlations.ipynb\"\n",
    "%run \"../Functions/Plot.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Online 1.52.2\n",
    "\n",
    "#gfdf = gfdfWebgl1522PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#rmdf = rmdfWebgl1522PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "\n",
    "### Playtest\n",
    "\n",
    "#gfdf = gfdfPlaytestTotalPretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#gfdf = gfdfPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#gfdf = gfdfPlaytestPhase2PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "\n",
    "#rmdf = rmdfPlaytestTotalPretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#rmdf = rmdfPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#rmdf = rmdfPlaytestPhase2PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "\n",
    "### Online 1.60\n",
    "\n",
    "#gfdf = gfdfWebgl160PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#rmdf = rmdfWebgl160PretestPosttestUniqueProfilesVolunteers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quicker allData switching.\n",
    "gfdf    =          gfdfPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "rmdf    =          rmdfPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "allData = allBinaryDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quicker allData switching.\n",
    "#gfdf    =          gfdfWebgl1522Timed.copy()\n",
    "#rmdf    =          rmdfWebgl1522Timed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Google form analysis\n",
    "<a id=gfdf />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample:               gform\")\n",
    "print(\"surveys:              %s\" % len(gform))\n",
    "print(\"unique users:         %s\" % getUniqueUserCount(gform))\n",
    "print(\"RM before:            %s\" % len(gform[gform[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before:            %s\" % len(getGFormBefores(gform)))\n",
    "print(\"RM after:             %s\" % len(gform[gform[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after:             %s\" % len(getGFormAfters(gform)))\n",
    "print(\"unique biologists:    %s\" % getUniqueUserCount(getSurveysOfBiologists(gform)))\n",
    "print(\"unique gamers:        %s\" % getUniqueUserCount(getSurveysOfGamers(gform)))\n",
    "print(\"unique perfect users: %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gform)))\n",
    "print(\"unique perfect users: %s\" % getPerfectPretestPostestPairsCount(gform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample:               gfdf\")\n",
    "print(\"surveys:              %s\" % len(gfdf))\n",
    "print(\"unique users:         %s\" % getUniqueUserCount(gfdf))\n",
    "print(\"RM before:            %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before:            %s\" % len(getGFormBefores(gfdf)))\n",
    "print(\"RM after:             %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after:             %s\" % len(getGFormAfters(gfdf)))\n",
    "print(\"unique biologists:    %s\" % getUniqueUserCount(getSurveysOfBiologists(gfdf)))\n",
    "print(\"unique gamers:        %s\" % getUniqueUserCount(getSurveysOfGamers(gfdf)))\n",
    "print(\"unique perfect users: %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gfdf)))\n",
    "print(\"unique perfect users: %s\" % getPerfectPretestPostestPairsCount(gfdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### formatted version for nice display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"category | count\")\n",
    "print(\"--- | ---\")\n",
    "print(\"sample | gform\")\n",
    "print(\"surveys | %s\" % len(gform))\n",
    "print(\"unique users | %s\" % getUniqueUserCount(gform))\n",
    "print(\"RM before | %s\" % len(gform[gform[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before | %s\" % len(getGFormBefores(gform)))\n",
    "print(\"RM after | %s\" % len(gform[gform[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after | %s\" % len(getGFormAfters(gform)))\n",
    "print(\"unique biologists | %s\" % getUniqueUserCount(getSurveysOfBiologists(gform)))\n",
    "print(\"unique gamers | %s\" % getUniqueUserCount(getSurveysOfGamers(gform)))\n",
    "print(\"unique perfect users | %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gform)))\n",
    "print(\"unique perfect users | %s\" % getPerfectPretestPostestPairsCount(gform))\n",
    "print()\n",
    "#print(\"(\" + str(pd.to_datetime('today').date()) + \")\")\n",
    "print(\"(\"+dataFilesNamesStem+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"category | count\")\n",
    "print(\"--- | ---\")\n",
    "print(\"sample | gfdf\")\n",
    "print(\"surveys | %s\" % len(gfdf))\n",
    "print(\"unique users | %s\" % getUniqueUserCount(gfdf))\n",
    "print(\"RM before | %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before | %s\" % len(getGFormBefores(gfdf)))\n",
    "print(\"RM after | %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after | %s\" % len(getGFormAfters(gfdf)))\n",
    "print(\"unique biologists | %s\" % getUniqueUserCount(getSurveysOfBiologists(gfdf)))\n",
    "print(\"unique gamers | %s\" % getUniqueUserCount(getSurveysOfGamers(gfdf)))\n",
    "print(\"unique perfect users | %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gfdf)))\n",
    "print(\"unique perfect users | %s\" % getPerfectPretestPostestPairsCount(gfdf))\n",
    "print()\n",
    "#print(\"(\" + str(pd.to_datetime('today').date()) + \")\")\n",
    "print(\"(\"+dataFilesNamesStem+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 complete sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(gfdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Per temporality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 answered only before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_befores = getGFormBefores(gfdf)\n",
    "rm_befores = getRMBefores(gfdf)\n",
    "gfrm_befores = getRMBefores(getGFormBefores(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gf_befores[QUserId] == rm_befores[QUserId]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gf_befores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 answered only after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_afters = getGFormAfters(gfdf)\n",
    "rm_afters = getRMAfters(gfdf)\n",
    "gfrm_afters = getRMAfters(getGFormBefores(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gf_afters[QUserId] == rm_afters[QUserId]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gf_afters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 answered both before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gf_both = getSurveysOfUsersWhoAnsweredBoth(gfdf, gfMode = True, rmMode = False)\n",
    "rm_both = getSurveysOfUsersWhoAnsweredBoth(gfdf, gfMode = False, rmMode = True)\n",
    "gfrm_both = getSurveysOfUsersWhoAnsweredBoth(gfdf, gfMode = True, rmMode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gf_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(rm_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gfrm_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 pretest vs posttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.4.1 phase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrixToDisplay = plotBasicStats(\n",
    "    gfdf,\n",
    "    horizontalPlot=False,\n",
    "    sortedAlong=\"\",\n",
    "    figsize=(12,20),\n",
    "    title = 'percentages of correct answers',\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 13},\n",
    "    font_scale=1.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrixToDisplay = plotBasicStats(\n",
    "    gfdf,\n",
    "    title = 'percentages of correct answers (sorted)',\n",
    "    sortedAlong=\"progression\",\n",
    "    horizontalPlot=False,\n",
    "    figsize=(12,20),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 13},\n",
    "    font_scale=1.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrixToDisplay.to_csv(\"../../data/sortedPrePostProgression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrixToDisplay.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Per demography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 English speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortEN = gfdf[gfdf[QLanguage] == enLanguageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 French speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortFR = gfdf[gfdf[QLanguage] == frLanguageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortFR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortF = gfdf[gfdf[QGender] == 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortM = gfdf[gfdf[QGender] == 'Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 biologists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortBioS = getSurveysOfBiologists(gfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortBioS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortBioB = getSurveysOfBiologists(gfdf, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortBioB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.6 gamers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortGamS = getSurveysOfGamers(gfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortGamS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortGamB = getSurveysOfGamers(gfdf, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortGamB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#T-tests between pretest and posttest scores among some player groups\n",
    "plotBasicStats(gfdf, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));\n",
    "plotBasicStats(cohortF, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));\n",
    "plotBasicStats(cohortM, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));\n",
    "plotBasicStats(cohortGamB, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 answered only after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 answers to scientific questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciBinarizedBefore = getAllBinarized(getRMBefores(gfdf))\n",
    "#sciBinarizedBefore = getAllBinarized(getGFBefores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        sciBinarizedBefore,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlations on survey questions before',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        sciBinarizedBefore,\n",
    "                        _abs=True,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciBinarizedAfter = getAllBinarized(getRMAfters(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        sciBinarizedAfter,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlations on survey questions after',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        sciBinarizedAfter,\n",
    "                        _abs=False,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisClustermap.ax_heatmap.annotate(overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir(thisClustermap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir(thisClustermap.ax_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vars(thisClustermap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vars(thisClustermap.ax_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 answers to all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allQuestions = correctAnswers + demographicAnswers\n",
    "\n",
    "allBinarized = getAllBinarized(gfdf, _source = allQuestions)\n",
    "allBinarizedBefore = getAllBinarized(getRMBefores(gfdf), _source = allQuestions)\n",
    "allBinarizedAfter = getAllBinarized(getRMAfters(gfdf), _source = allQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        allBinarized,\n",
    "                        _abs=True,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlation of all answers',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        allBinarizedAfter,\n",
    "                        _abs=True,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 answers to all questions, only before having played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        allBinarizedBefore,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlations on all questions before',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        allBinarizedBefore,\n",
    "                        _abs=True,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 answers to all questions, only after having played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(\n",
    "                        allBinarizedAfter,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlation of all answers after',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBinarizedAfterSub = allBinarizedAfter.copy()\n",
    "allBinarizedAfterSub = allBinarizedAfterSub.loc[:,['Age'] + scientificQuestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(\n",
    "                        allBinarizedAfterSub,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlation of all answers after',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Game sessions\n",
    "<a id=sessions />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#startDate = minimum152Date\n",
    "#endDate = maximum152Date\n",
    "\n",
    "startDate = rmdf['userTime'].min().date() - datetime.timedelta(days=1)\n",
    "endDate = rmdf['userTime'].max().date() + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = rmdf['userTime'].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='RedMetrics events', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = rmdf[rmdf['type'] == 'start']['userTime'].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='sessions', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = rmdf.groupby('userId').agg({ \"userTime\": np.min })['userTime'].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='game users', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = gfdf.groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='survey answers', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforesPerDay = gfdf[gfdf[QTemporality] == answerTemporalities[0]].groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "aftersPerDay = gfdf[gfdf[QTemporality] == answerTemporalities[1]].groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "undefinedPerDay = gfdf[gfdf[QTemporality] == answerTemporalities[2]].groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "\n",
    "plotPerDay(beforesPerDay, title='survey befores', startDate=startDate, endDate=endDate)\n",
    "plotPerDay(aftersPerDay, title='survey afters', startDate=startDate, endDate=endDate)\n",
    "plotPerDay(undefinedPerDay, title='survey undefined', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Per session and per user analysis\n",
    "<a id=peruser />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. User comparison\n",
    "<a id=usercomp />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do: transfer part of 1.3's \"'Google form analysis' functions tinkering\" code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## percentagesCrossCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretests = gform[gform[QTemporality] == answerTemporalities[0]]\n",
    "#pretests[pretests[QBBFunctionPlasmid] == ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized = sciBinarizedBefore\n",
    "intermediaryNumerator = getCrossCorrectAnswers(binarized).round().astype(int)*100\n",
    "percentagesCrossCorrect = (intermediaryNumerator / binarized.shape[0]).round().astype(int)\n",
    "totalPerQuestion = np.dot(np.ones(binarized.shape[0]), binarized)\n",
    "sciBinarizedBefore.columns[totalPerQuestion == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getPercentageCrossCorrect(sciBinarizedBefore, figsize=(40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPercentageCrossCorrect(sciBinarizedAfter, figsize=(40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gfdf), len(getAllResponders(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrixToDisplay = plotBasicStats(gfdf, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectCount = allData.shape[1]\n",
    "measuredPretest = 100*allData.loc[pretestScientificQuestions,:].sum(axis='columns')/subjectCount\n",
    "measuredPretest.index = scientificQuestions\n",
    "measuredPosttest = 100*allData.loc[posttestScientificQuestions,:].sum(axis='columns')/subjectCount\n",
    "measuredPosttest.index = scientificQuestions\n",
    "measuredDelta2 = (measuredPosttest - measuredPretest)\n",
    "measuredDelta2 = pd.DataFrame(measuredDelta2.round().astype(int))\n",
    "measuredDelta2.columns = [\"measuredDelta2\"]\n",
    "measuredDelta2 = measuredDelta2.sort_values(by = \"measuredDelta2\", ascending = True).T\n",
    "_fig = plt.figure(figsize=(20,2))\n",
    "_ax1 = plt.subplot(111)\n",
    "_ax1.set_title(\"measuredDelta2\")\n",
    "sns.heatmap(\n",
    "            measuredDelta2,\n",
    "            ax=_ax1,\n",
    "            cmap=plt.cm.jet,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            vmin=0,\n",
    "            vmax=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(matrixToDisplay.loc['progression',scientificQuestions] - measuredDelta2.loc['measuredDelta2',scientificQuestions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.DataFrame(columns=[\n",
    "    'pretest1', 'posttest1', 'measuredDelta',\n",
    "    'pretest2', 'posttest2', 'matrixToDisplay'], data = 0, index= scientificQuestions)\n",
    "testDF['pretest1'] = measuredPretest\n",
    "testDF['posttest1'] = measuredPosttest\n",
    "testDF['measuredDelta'] = measuredDelta2.T['measuredDelta2']\n",
    "testDF['pretest2'] = matrixToDisplay.T['pretest'][scientificQuestions]\n",
    "testDF['posttest2'] = matrixToDisplay.T['posttest'][scientificQuestions]\n",
    "testDF['matrixToDisplay'] = matrixToDisplay.T['progression'][scientificQuestions]\n",
    "testDF = testDF.round().astype(int)\n",
    "#testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuredDelta = allData.loc[deltaScientificQuestions,:].sum(axis='columns')\n",
    "measuredDelta.mean(), measuredDelta.median()\n",
    "#measuredDelta.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pretestData = getAllUserVectorData( gfdf[gfdf[QTemporality] == answerTemporalities[0]], _source = correctAnswers )\n",
    "#posttestData = getAllUserVectorData( gfdf[gfdf[QTemporality] == answerTemporalities[1]], _source = correctAnswers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAllUserVectorDataCorrelationMatrix(\n",
    "    allData.T,\n",
    "    _abs=False,\n",
    "    _figsize = (40,40),\n",
    "    _clustered=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demographicCriteria = demographicQuestions.copy()\n",
    "\n",
    "plotAllUserVectorDataCorrelationMatrix(\n",
    "    allData.T,\n",
    "    _abs=False,\n",
    "    _figsize = (20,20),\n",
    "    _clustered=False,\n",
    "    columnSubset=[]\\\n",
    "        + completionTimesCriteria\n",
    "        + totalTimesCriteria\n",
    "        + pretestScientificQuestions\n",
    "        #+ posttestScientificQuestions\n",
    "        #+ deltaScientificQuestions\n",
    "        + overallScoreCriteria\n",
    "        #+ demographicCriteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completers = rmdf[rmdf['type'] == 'complete'][QUserId]\n",
    "#nonCompleter = rmdf[~rmdf[QUserId].isin(completers)][QUserId].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getUserDataVector(nonCompleter)#.loc[14,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allData.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# completed vs played time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(index=allData.columns, columns=[\"time\", \"posttestScore\", \"deltaScore\",\"completed\"])\n",
    "for userId in data.index:\n",
    "    data.loc[userId, \"time\"] = getPlayedTimeUser(userId, _rmDF = rmdf)['tutorial']['totalSpentTime'].total_seconds()\n",
    "    data.loc[userId, \"posttestScore\"] = allData.loc['scoreposttest', userId]\n",
    "    data.loc[userId, \"pretestScore\"] = allData.loc['scorepretest', userId]\n",
    "    data.loc[userId, \"deltaScore\"] = allData.loc['scoredelta', userId]\n",
    "    data.loc[userId, \"completed\"] = allData.loc['complete', userId]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = allScores.copy()\n",
    "x2 = completedScores.copy()\n",
    "y = allPlayedTimes.copy()\n",
    "y2 = completedPlayedTimes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotDF = pd.DataFrame(index = x.index, data = x)\n",
    "plotDF['times'] = y\n",
    "#plotDF\n",
    "#(plotDF['times'] == y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"posttestScore\"]\n",
    "x2 = data[data[\"completed\"]==1][\"posttestScore\"]\n",
    "y = data[\"time\"]\n",
    "y2 = data[data[\"completed\"]==1][\"time\"]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "ax1 = plt.subplot(121)\n",
    "plt.scatter(x, y)#, c='blue', alpha=0.5)\n",
    "plt.scatter(x2, y2)#, c='red', alpha=0.5)\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('time')\n",
    "plt.title(\"time against score, n=\" + str(len(x)))\n",
    "#ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.scatter(y, x)\n",
    "plt.scatter(y2, x2)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('score')\n",
    "plt.title(\"score against time, n=\" + str(len(x)))\n",
    "ax2.legend(loc='center left', bbox_to_anchor=(-1.2, 0.9), labels =[\"unfinished games\",\"completed games\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"posttestScore\"].astype(float)\n",
    "x2 = data[data[\"completed\"]==1][\"posttestScore\"].astype(float)\n",
    "y = data[\"time\"].astype(float)\n",
    "y2 = data[data[\"completed\"]==1][\"time\"].astype(float)\n",
    "\n",
    "# Get the linear models\n",
    "lm_original = np.polyfit(x, y, 1)\n",
    " \n",
    "# calculate the y values based on the co-efficients from the model\n",
    "r_x, r_y = zip(*((i, i*lm_original[0] + lm_original[1]) for i in x))\n",
    " \n",
    "# Put in to a data frame, to keep is all nice\n",
    "lm_original_plot = pd.DataFrame({\n",
    "'scores' : r_x,\n",
    "'times' : r_y\n",
    "})\n",
    "\n",
    "lm_original_plot = lm_original_plot.drop_duplicates()\n",
    "lm_original_plot = lm_original_plot.sort_values(by=\"scores\")\n",
    "lm_original_plot = lm_original_plot.drop(lm_original_plot.index[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot(111)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x2, y2)\n",
    "# Plot the original data and model\n",
    "#lm_original_plot.plot(kind='line', color='Red', x='scores', y='times', ax=ax)\n",
    "plt.plot('scores', 'times', data=lm_original_plot, color='Red')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('time') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x = data[\"posttestScore\"].astype(float)\n",
    "x2 = data[data[\"completed\"]==1][\"posttestScore\"].astype(float)\n",
    "y = data[\"time\"].astype(float)\n",
    "y2 = data[data[\"completed\"]==1][\"time\"].astype(float)\n",
    "\n",
    "xReshaped = x.values.reshape(-1, 1)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(xReshaped, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred = regr.predict(xReshaped)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y, pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(x, y, color='black')\n",
    "plt.plot(x, pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.intercept_,regr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, color=\"b\")\n",
    "plt.scatter(x2, y2, color='red')\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"time played\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.DataFrame(index = range(0, len(xReshaped)), data = xReshaped, columns = ['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['time'] = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data2 = data.loc[:, [\"time\", \"posttestScore\"]]\n",
    "data2.index = range(0, data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import patsy\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "timeScoreformula = 'time ~ posttestScore'\n",
    "\n",
    "# create a fitted model\n",
    "lm1 = smf.ols(formula=timeScoreformula, data=data2).fit()\n",
    "\n",
    "# print the coefficients\n",
    "#lm1.params\n",
    "\n",
    "\n",
    "#lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confidence intervals for the model coefficients\n",
    "lm1.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the p-values for the model coefficients\n",
    "# Represents the probability that the coefficient is actually zero\n",
    "lm1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the R-squared value for the model\n",
    "lm1.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed vs non-completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "timeScoreformula = 'time ~ posttestScore'\n",
    "lm1 = smf.ols(formula=timeScoreformula, data=data2).fit()\n",
    "lm2 = smf.ols(formula=timeScoreformula, data=data2[data2[\"completed\"] == 0]).fit()\n",
    "lm3 = smf.ols(formula=timeScoreformula, data=data2[data2[\"completed\"] == 1]).fit()\n",
    "lm1.rsquared,lm2.rsquared,lm3.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['deltaScoreRate'] = data['deltaScore']/data['pretestScore']\n",
    "meanDelta = data['deltaScore'].mean()\n",
    "meanPretest = data['pretestScore'].mean()\n",
    "meanDelta/meanPretest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations between durations and score on questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlations between completion time of checkpoint n and score on question Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallScoreCriteria = [\"scorepretest\", \"scoreposttest\", \"scoredelta\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemTimesCriteria = [\"ch\" + \"{0:0=2d}\".format(i) for i in range(0,15)]\n",
    "completionTimesCriteria = [st + \"completion\" for st in stemTimesCriteria] + [\"completionTime\"]\n",
    "totalTimesCriteria = [st + \"total\" for st in stemTimesCriteria] + [\"totalTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData2 = allData.T.rename(str,axis=\"columns\")\n",
    "allData3 = allData2[allData2['ch00completion'] < pd.Timedelta.max.total_seconds()]\n",
    "len(allData3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData2[allData2[criterionLabel]>9e+09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sectionNb = '01'\n",
    "criterionLabel = 'ch' + sectionNb + 'completion'\n",
    "sectionName = 'tutorial.Checkpoint' + sectionNb\n",
    "testUserId = allData2[allData2[criterionLabel]>9e+09].index[0]\n",
    "#rmdf or rmdfConcat\n",
    "_rmdf = rmdfConcat\n",
    "_rmdf[(_rmdf[QUserId] == testUserId) \\\n",
    "           & (_rmdf['type'] == 'reach') \\\n",
    "           & (_rmdf['section'] == 'tutorial.Checkpoint' + sectionNb) \\\n",
    "          ].loc[:, ['section', 'userTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rmdf[(_rmdf[QUserId] == testUserId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdf[gfdf[QUserId] == testUserId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#chosenPrefix = answerTemporalities[0]\n",
    "chosenPrefix = answerTemporalities[1]\n",
    "#chosenPrefix = \"delta\"\n",
    "\n",
    "#warning: not the same as displayed columns, see lower\n",
    "chosenCriteria = [chosenPrefix + \" \" + q for q in scientificQuestions] + overallScoreCriteria\n",
    "\n",
    "durationsScoresCorrelations = pd.DataFrame(index=completionTimesCriteria+totalTimesCriteria, columns=chosenCriteria, data=np.nan)\n",
    "durationsScoresCorrelations = durationsScoresCorrelations.rename(str, axis='rows')\n",
    "annotationMatrix = np.empty(shape=[durationsScoresCorrelations.shape[0], 1], dtype=int)\n",
    "#annotationMatrix2D = np.empty(durationsScoresCorrelations.shape, dtype=str)\n",
    "\n",
    "allData2 = allData.T.rename(str,axis=\"columns\")\n",
    "for i in range(len(durationsScoresCorrelations.index)):\n",
    "    checkpoint = durationsScoresCorrelations.index[i]\n",
    "    allData3 = allData2[allData2[checkpoint] < pd.Timedelta.max.total_seconds()]\n",
    "    annotationMatrix[i] = len(allData3)\n",
    "    for q in durationsScoresCorrelations.columns:\n",
    "        corr = np.corrcoef(allData3[checkpoint], allData3[q])\n",
    "        if corr[0,0] < 0:\n",
    "            print(\"[\" + checkpoint + \";\" + q + \"]:\" + str(corr[0,0]))\n",
    "        #if pd.isnull(corr[0,1]):\n",
    "        #    print(\"[\" + checkpoint + \";\" + q + \"] null\")\n",
    "        durationsScoresCorrelations.loc[checkpoint, q] = corr[0,1]\n",
    "        \n",
    "_fig, (_a0, _a1) = plt.subplots(1,2, gridspec_kw = {'width_ratios':[50, 1]}, figsize=(15,10))\n",
    "\n",
    "#_a0.set_title(\"correlations between times and \" + chosenPrefix + \" scores\")\n",
    "_a0.set_title(\"correlations between times and scores\")\n",
    "\n",
    "durationsScoresCorrelations.columns = [q for q in scientificQuestions] + [\"pretest score\", \"posttest score\", \"score increase\",]\n",
    "sns.heatmap(durationsScoresCorrelations, ax=_a0, cmap=plt.cm.jet, square=True, vmin=-1, vmax=1,\n",
    "            # annot=True,\n",
    "            # annot=annotationMatrix2D\n",
    "            #cbar_kws= {'panchor':(0.0, 0.0)}\n",
    "           )\n",
    "\n",
    "_a1.set_title(\"\")\n",
    "sns.heatmap(annotationMatrix, ax=_a1, annot=annotationMatrix)\n",
    "\n",
    "_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#chosenPrefix = answerTemporalities[0]\n",
    "#chosenPrefix = answerTemporalities[1]\n",
    "#chosenPrefix = \"delta\"\n",
    "\n",
    "#warning: not the same as displayed columns, see lower\n",
    "#questions1 = [QAge,QGender]\n",
    "#questions1 = [QEnjoyed]\n",
    "\n",
    "#questions2 = [\n",
    "#    QCuriosityBiology,QCuriositySyntheticBiology,QCuriosityVideoGames,\n",
    "#    QCuriosityEngineering,\n",
    "##    QPlayed,\n",
    "#    QAge,QGender,\n",
    "#    QInterestVideoGames,\n",
    "#    QInterestBiology,QStudiedBiology,QPlayVideoGames,\n",
    "##    QHeardSynBioOrBioBricks,\n",
    "##    QVolunteer,\n",
    "#    QEnjoyed]\n",
    "\n",
    "#questions2 = [\n",
    "#    QCuriosityBiology,\n",
    "#    QCuriositySyntheticBiology,\n",
    "#    QCuriosityVideoGames,\n",
    "#    QCuriosityEngineering,\n",
    "#    QPlayed,\n",
    "    #QAge,\n",
    "    #QGender,\n",
    "#    QInterestVideoGames,\n",
    "#    QInterestBiology,\n",
    "#    QStudiedBiology,\n",
    "#    QPlayVideoGames,\n",
    "#    QHeardSynBioOrBioBricks,\n",
    "#    QVolunteer,\n",
    "#    QEnjoyed #use only posttest value\n",
    "#    ]\n",
    "\n",
    "questions2 = [\n",
    "    QCuriosityEngineering,\n",
    "    \n",
    "    QCuriosityBiology,\n",
    "    QCuriositySyntheticBiology,\n",
    "    QInterestBiology,\n",
    "    QStudiedBiology,\n",
    "    \n",
    "    QCuriosityVideoGames,\n",
    "    QInterestVideoGames,\n",
    "    QPlayVideoGames,\n",
    "    \n",
    "#    QPlayed,\n",
    "    QAge,\n",
    "    QGender,\n",
    "#    QHeardSynBioOrBioBricks,\n",
    "#    QVolunteer,\n",
    "#    QEnjoyed #use only posttest value\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "#chosenCriteria1 = completionTimesCriteria+totalTimesCriteria\n",
    "chosenCriteria1 = [\"posttest \" + q for q in scientificQuestions] + overallScoreCriteria\n",
    "#chosenCriteria1 = [\"pretest \" + q for q in questions1]\n",
    "#chosenCriteria1 = [\"posttest \" + q for q in questions1]\n",
    "\n",
    "#chosenCriteria2 = [\"posttest \" + q for q in questions2]\n",
    "#chosenCriteria2 = [\"pretest \" + q for q in questions2] + [\"posttest \" + QEnjoyed]\n",
    "#chosenCriteria2 = [\"posttest \" + q for q in scientificQuestions] + overallScoreCriteria\n",
    "#chosenCriteria2 = [\"pretest \" + q for q in questions2]\n",
    "chosenCriteria2 = [\"maxChapter\"]\n",
    "\n",
    "criteriaScoresCorrelations = pd.DataFrame(index=chosenCriteria1, columns=chosenCriteria2, data=np.nan)\n",
    "criteriaScoresCorrelations = criteriaScoresCorrelations.rename(str, axis='rows')\n",
    "annotationMatrix = np.empty(shape=[criteriaScoresCorrelations.shape[0], 1], dtype=int)\n",
    "#annotationMatrix2D = np.empty(durationsScoresCorrelations.shape, dtype=str)\n",
    "\n",
    "#allData2 = allData.T.rename(str,axis=\"columns\")\n",
    "#allData2 = allBinaryDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers\n",
    "allData2 = allNumericDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers\n",
    "allData2 = allData2.T.rename(str,axis=\"columns\")\n",
    "\n",
    "for i in range(len(criteriaScoresCorrelations.index)):\n",
    "    criterion1i  = criteriaScoresCorrelations.index[i]\n",
    "    \n",
    "    allData3 = allData2\n",
    "    if criterion1i in completionTimesCriteria:\n",
    "        allData3 = allData2[allData2[criterion1i] < pd.Timedelta.max.total_seconds()]\n",
    "    annotationMatrix[i] = len(allData3)\n",
    "    \n",
    "    for criterion2j in criteriaScoresCorrelations.columns:\n",
    "        corr = np.corrcoef(allData3[criterion1i], allData3[criterion2j])\n",
    "        if corr[0,0] < 0:\n",
    "            print(\"[\" + criterion1i + \";\" + criterion2j + \"]:\" + str(corr[0,0]))\n",
    "        #if pd.isnull(corr[0,1]):\n",
    "        #    print(\"[\" + criterion1i + \";\" + criterion2j + \"] null\")\n",
    "        criteriaScoresCorrelations.loc[criterion1i, criterion2j] = corr[0,1]\n",
    "        \n",
    "#index 1\n",
    "#criteriaScoresCorrelations.index = scientificQuestions + [\"pretest score\", \"posttest score\", \"score increase\"]\n",
    "#criteriaScoresCorrelations.index = questions1\n",
    "#columns 2\n",
    "#criteriaScoresCorrelations.columns = questions2\n",
    "#criteriaScoresCorrelations.columns = questions2 + [QEnjoyed]\n",
    "#criteriaScoresCorrelations.columns = scientificQuestions + [\"pretest score\", \"posttest score\", \"score increase\"]\n",
    "criteriaScoresCorrelations.columns = [\"max. checkpoint\"]\n",
    "\n",
    "# (10,20) big\n",
    "# (12,5) small\n",
    "#_fig, (_a0) = plt.subplots(1,1, figsize=(10,18))\n",
    "_fig, (_a0) = plt.subplots(1,1, figsize=(6,10))\n",
    "#_fig, (_a0, _a1) = plt.subplots(\n",
    "#    1,2, figsize=(5,25), gridspec_kw = {'width_ratios':[15, 1]})\n",
    "#    2,1, figsize=(17,12), gridspec_kw = {'height_ratios':[30, 1]})\n",
    "\n",
    "#sns.set(font_scale=1)\n",
    "sns.set(font_scale=1.3)\n",
    "#sns.set(font_scale=1.7)\n",
    "data = criteriaScoresCorrelations\n",
    "\n",
    "#_a0.set_title(\"correlations between times and demographic criteria\")\n",
    "#_a0.set_title(\"correlations between scores and demographic criteria\")\n",
    "#_a0.set_title(\"correlations between (age, gender) and (curiosity, interest, practice, enjoyment)\")\n",
    "#_a0.set_title(\"correlations between enjoyment and age, gender, curiosity, interest, practice, enjoyment\")\n",
    "#plt.title(\"correlations between enjoyment and age, gender, curiosity, interest, practice\")\n",
    "#_a0.set_title(\"correlations between times and scores\")\n",
    "_a0.set_title(\"correlations between scores and maximum checkpoint reached\")\n",
    "\n",
    "_a0.set_anchor('C')\n",
    "sns.heatmap(data, ax=_a0, cmap=plt.cm.jet, square=True, vmin=-1, vmax=1,\n",
    "            # annot=True,\n",
    "            # cbar = False,\n",
    "            # annot=annotationMatrix2D\n",
    "            #cbar_kws= {'panchor':(0.0, 0.0)}\n",
    "            #cbar_kws = dict(use_gridspec=False,location=\"right\"),\n",
    "            annot_kws={\"size\": 13},\n",
    "            #annot_kws={\"size\": 13},\n",
    "           )\n",
    "\n",
    "#_a1.set_anchor('C')\n",
    "#data = annotationMatrix.T\n",
    "#sns.heatmap(data, ax=_a1, annot=data, square=True,\n",
    "#             cbar = False,xticklabels=False,yticklabels=False,annot_kws={\"size\": 12})\n",
    "\n",
    "_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fig, (_a1) = plt.subplots(1,1, figsize=(10,5))\n",
    "_a1.set_anchor('C')\n",
    "#data = pd.Series(data=annotationMatrix.flatten(), index=completionTimesCriteria+totalTimesCriteria)\n",
    "data = annotationMatrix.T\n",
    "sns.heatmap(data, \n",
    "            ax=_a1,\n",
    "            annot=data,\n",
    "            square=True,\n",
    "             cbar = False,\n",
    "            #xticklabels=False,\n",
    "            xticklabels=completionTimesCriteria+totalTimesCriteria,\n",
    "            yticklabels=False,\n",
    "            #yticklabels=completionTimesCriteria+totalTimesCriteria,\n",
    "            annot_kws={\"size\": 12})\n",
    "\n",
    "_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "checkpoint = durationsScoresCorrelations.index[i]\n",
    "print(checkpoint + \": \" + str(len(allData2[allData2[checkpoint] < pd.Timedelta.max.total_seconds()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUserId = gfdf[QUserId].unique()[12]\n",
    "getCheckpointsTotalTimesUser(testUserId, rmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timedSectionnedEvents.to_csv(\"ch4.csv\", encoding=csvEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getAllResponders(gfdf), _source = correctAnswers, _rmDF = rmdf\n",
    "#testUserId = \"4731525f-62dd-4128-ab56-3991b403e17e\"\n",
    "#getUserDataVector(testUserId,_source = correctAnswers, _rmDF = rmdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max chapter vs scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta or posttest?\n",
    "# posttest: values 0, 1 managed in plotCorrectedAnswerPerMaxCheckpoint\n",
    "# delta can't work: values 0, 1 and -1 not managed in plotCorrectedAnswerPerMaxCheckpoint\n",
    "chosenPrefix = \"posttest\"\n",
    "chosenQuestions = [chosenPrefix + \" \" + q for q in scientificQuestions]\n",
    "criteria = [\"maxChapter\",\"complete\"] + chosenQuestions + overallScoreCriteria\n",
    "\n",
    "#data = allBinaryDataPlaytestPhase1PretestPosttestUniqueProfiles.loc[criteria,:]\n",
    "data = allBinaryDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.loc[criteria,:]\n",
    "\n",
    "#data = allBinaryDataPlaytestPhase2PretestPosttestUniqueProfiles.loc[criteria,:]\n",
    "#data = allBinaryDataPlaytestPhase2PretestPosttestUniqueProfilesVolunteers.loc[criteria,:]\n",
    "\n",
    "#data = allNumericDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.loc[criteria,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = chosenPrefix + ' Function - biology: CDS'\n",
    "criterion2 = chosenPrefix + ' Device: RBS:PCONS:FLHDC:TER XXX'\n",
    "xIndex = 'maxChapter'\n",
    "dataT = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCheckpointThreshold(bars0, bars1, thresholdRatio = .9):\n",
    "    totalCount = np.sum(bars0) + np.sum(bars1)\n",
    "    cumulative0 = np.cumsum(list(reversed(bars0)))\n",
    "    cumulative1 = np.cumsum(list(reversed(bars1)))\n",
    "    result = 0\n",
    "    #np.argmax(cumulative1>=thresholdCount)\n",
    "    for i in range(len(cumulative1)):\n",
    "        thresholdCount = np.floor(thresholdRatio * (cumulative0[i] + cumulative1[i]))\n",
    "        if cumulative1[i] < thresholdCount:\n",
    "            result = 15-i\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    " \n",
    "# stacked horizontal bar plot; cf df.plot.barh?\n",
    "def plotCorrectedAnswerPerMaxCheckpoint(dataT, criterion, saveFig=False, plotFig=True, thresholdRatio=0.9):\n",
    "    # y-axis in bold\n",
    "    rc('font', weight='bold')\n",
    "\n",
    "    # Values of each group\n",
    "\n",
    "    bars0 = [len(dataT[(dataT[criterion]==0) & (dataT[xIndex]==maxChapterValue)]) for maxChapterValue in range(15)]\n",
    "    bars1 = [len(dataT[(dataT[criterion]==1) & (dataT[xIndex]==maxChapterValue)]) for maxChapterValue in range(15)]\n",
    "\n",
    "    if plotFig:\n",
    "        # Heights of bars1 + bars2 (TO DO better)\n",
    "        bars = [bars0[i] + bars1[i] for i in range(len(bars0))]\n",
    "\n",
    "        # The position of the bars on the x-axis\n",
    "        r = [i for i in range(15)]\n",
    "\n",
    "        # Names of group and bar width\n",
    "        names = [i for i in range(15)]\n",
    "        barWidth = 1\n",
    "\n",
    "        fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "        # Create red bars\n",
    "        ax.bar(r, bars0, color='#cc0c28', edgecolor='white', width=barWidth)\n",
    "        # Create green bars (middle), on top of the firs ones\n",
    "        ax.bar(r, bars1, bottom=bars0, color='#557f2d', edgecolor='white', width=barWidth)\n",
    "\n",
    "        # Custom X axis\n",
    "        plt.xticks(r, names, fontweight='bold')\n",
    "        plt.xlabel(\"max. checkpoint\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.title(\"Answers to question '\" + criterion + \"' against max. checkpoint, n=\" + str(len(dataT.index)))\n",
    "        ax.legend([\"incorrect\", \"correct\"], \n",
    "                    bbox_to_anchor=(0.7, 0.7),\n",
    "    #                loc=\"upper center\",\n",
    "                )\n",
    "\n",
    "        # Show graphic\n",
    "        plt.show()\n",
    "\n",
    "        if saveFig:\n",
    "            #correctedAnswersPerMaxCheckpoint\n",
    "            questionTitle = \"cAPMC-'\" + criterion.replace(\" \", \"_\").replace(\":\", \"\") + \"'\"\n",
    "            try:\n",
    "                fig.savefig(questionTitle)\n",
    "            except:\n",
    "                print(\"- savefig failed for \" + questionTitle)\n",
    "            \n",
    "    return [bars0, bars1, getCheckpointThreshold(bars0, bars1, thresholdRatio)]\n",
    "\n",
    "[bars0, bars1, threshold] = plotCorrectedAnswerPerMaxCheckpoint(dataT, criterion2, saveFig=False, plotFig=True)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCheckpointThreshold(bars0, bars1, thresholdRatio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(bars1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdsCheckpoints = pd.Series(index = chosenQuestions, data = 15, name = \"thresholdsCheckpoints\")\n",
    "\n",
    "for criterion in chosenQuestions:\n",
    "    [bars0, bars1, threshold] = plotCorrectedAnswerPerMaxCheckpoint(\n",
    "        dataT,\n",
    "        criterion,\n",
    "        saveFig=False,\n",
    "        plotFig=False,\n",
    "        thresholdRatio=0.8\n",
    "    )\n",
    "    thresholdsCheckpoints[criterion] = threshold\n",
    "thresholdsCheckpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdsCheckpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotCheckpointsFromThreshold(dataT, criterion, saveFig=False):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x in np.linspace(0.5,1,11):\n",
    "        [bars0, bars1, thresholdCheckpoint] = plotCorrectedAnswerPerMaxCheckpoint(\n",
    "            dataT,\n",
    "            criterion,\n",
    "            saveFig=False,\n",
    "            plotFig=False,\n",
    "            thresholdRatio=x\n",
    "        )\n",
    "        xs += [x]\n",
    "        ys += [thresholdCheckpoint]\n",
    "        #print(\"x=\" + str(x) +\": \" + str(thresholdCheckpoint))\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax1 = plt.subplot(111)\n",
    "    plt.plot(xs, ys)\n",
    "    plt.ylim((-0.5, 14.5))\n",
    "    plt.xlabel('threshold')\n",
    "    plt.ylabel('checkpoint')\n",
    "    plt.title(\"Checkpoint against threshold, for question '\" + criterion + \"'\")\n",
    "    plt.show()\n",
    "    \n",
    "    if saveFig:\n",
    "            #correctedAnswersPerMaxCheckpoint\n",
    "            questionTitle = \"cFT-'\" + criterion.replace(\" \", \"_\").replace(\":\", \"\") + \"'\"\n",
    "            try:\n",
    "                fig.savefig(questionTitle)\n",
    "            except:\n",
    "                print(\"- savefig failed for \" + questionTitle)\n",
    "    return ys\n",
    "                \n",
    "ys = plotCheckpointsFromThreshold(dataT, criterion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostFrequentThreshold(ys):\n",
    "    result = [x for x in ys if ((x != 15) & (x != 0))]\n",
    "    if len(result) == 0:\n",
    "        return Counter(ys).most_common(1)[0]\n",
    "    else:\n",
    "        return Counter(result).most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "thresholdsCheckpoints2 = pd.DataFrame(index = chosenQuestions, columns = ['threshold', 'count'], data = 15)\n",
    "\n",
    "for criterion in chosenQuestions:\n",
    "    ys = plotCheckpointsFromThreshold(dataT, criterion, saveFig=False)\n",
    "    thresholdsCheckpoints2.loc[criterion, 'threshold'] = getMostFrequentThreshold(ys)[0]\n",
    "    thresholdsCheckpoints2.loc[criterion, 'count'] = getMostFrequentThreshold(ys)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdsCheckpoints2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for criterion in criteria:\n",
    "criterion = 'scoreposttest'\n",
    "x = data.loc[\"maxChapter\",:].values\n",
    "y = data.loc[criterion,:].values\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax1 = plt.subplot(111)\n",
    "plt.scatter(x, y)#, c='blue', alpha=0.5)\n",
    "plt.xlabel('max. checkpoint')\n",
    "plt.ylabel(\"posttest score\")\n",
    "plt.title(\"Posttest score against max. checkpoint, n=\" + str(len(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, color=\"b\", x_estimator=np.mean)\n",
    "plt.xlabel(\"max. checkpoint\")\n",
    "plt.ylabel(\"posttest score\")\n",
    "plt.title(\"Posttest score against max. checkpoint, n=\" + str(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import patsy\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = data.T.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "scoreCheckpointformula = criterion + ' ~ maxChapter'\n",
    "\n",
    "# create a fitted model\n",
    "lm1 = smf.ols(formula=scoreCheckpointformula, data=dataT).fit()\n",
    "\n",
    "# print the coefficients\n",
    "#lm1.params\n",
    "\n",
    "\n",
    "#lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confidence intervals for the model coefficients\n",
    "lm1.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the p-values for the model coefficients\n",
    "# Represents the probability that the coefficient is actually zero\n",
    "lm1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the R-squared value for the model\n",
    "lm1.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "#x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15], dtype=float)\n",
    "#y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])\n",
    "\n",
    "def piecewise_linear(x, x0, y0, k1, k2):\n",
    "    return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])\n",
    "\n",
    "p , e = optimize.curve_fit(piecewise_linear, x, y)\n",
    "xd = np.linspace(0, 14, 100)\n",
    "plt.plot(x, y, \"o\")\n",
    "plt.plot(xd, piecewise_linear(xd, *p))\n",
    "plt.xlabel(\"max. checkpoint\")\n",
    "plt.ylabel(\"posttest score\")\n",
    "# piecewise regression\n",
    "plt.title(\"Posttest score against max. checkpoint, segmented regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Game map\n",
    "<a id=map />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#players = rmdf.loc[:, playerFilteringColumns]\n",
    "players = safeGetNormalizedRedMetricsCSV( rmdf )\n",
    "players.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#players = players.dropna(how='any')\n",
    "#players.head(1)\n",
    "#rmdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#players = players[~players['userId'].isin(excludedIDs)];\n",
    "#players.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions (filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionscount = players[\"sessionId\"].nunique()\n",
    "sessionscount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions of dev IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueplayers = players['userId']\n",
    "uniqueplayers = uniqueplayers.unique()\n",
    "uniqueplayers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#uniqueplayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueplatforms = players['customData.platform'].unique()\n",
    "uniqueplatforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints passed / furthest checkpoint (unfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = rmdf.loc[:, ['type', 'section', 'sessionId']]\n",
    "checkpoints = checkpoints[checkpoints['type']=='reach'].loc[:,['section','sessionId']]\n",
    "checkpoints = checkpoints[checkpoints['section'].str.startswith('tutorial', na=False)]\n",
    "checkpoints = checkpoints.groupby(\"sessionId\")\n",
    "checkpoints = checkpoints.max()\n",
    "#len(checkpoints)\n",
    "checkpoints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxCheckpointTable = pd.DataFrame({\"maxCheckpoint\" : checkpoints.values.flatten()})\n",
    "maxCheckpointCounts = maxCheckpointTable[\"maxCheckpoint\"].value_counts()\n",
    "maxCheckpointCounts['Start'] = None\n",
    "maxCheckpointCounts = maxCheckpointCounts.sort_index()\n",
    "print('\\nmaxCheckpointCounts=\\n{0}'.format(str(maxCheckpointCounts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxCheckpointCountsTable = pd.DataFrame({\"maxCheckpoint\" : maxCheckpointCounts.values})\n",
    "maxCheckpointCountsTableCount = maxCheckpointCountsTable.sum(0)[0]\n",
    "maxCheckpointCountsTableCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoints.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCheckpointCountsTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCheckpointCountsTable.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genericTreatment( maxCheckpointCountsTable, \"best checkpoint reached\", \"game sessions\", 0, maxCheckpointCountsTableCount, False, True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts = rmdf.loc[:, checkpointsRelevantColumns]\n",
    "#starts = checkpoints[checkpoints['type']=='start'].loc[:,['playerId']]\n",
    "#starts = checkpoints[checkpoints['section'].str.startswith('tutorial', na=False)]\n",
    "#starts = checkpoints.groupby(\"playerId\")\n",
    "#starts = checkpoints.max()\n",
    "#starts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTutorial1Count = sessionscount\n",
    "neverReachedGameSessionCount = startTutorial1Count - maxCheckpointCountsTableCount\n",
    "fullMaxCheckpointCounts = maxCheckpointCounts\n",
    "fullMaxCheckpointCounts['Start'] = neverReachedGameSessionCount\n",
    "fullMaxCheckpointCountsTable = pd.DataFrame({\"fullMaxCheckpoint\" : fullMaxCheckpointCounts.values})\n",
    "\n",
    "genericTreatment( fullMaxCheckpointCountsTable, \"best checkpoint reached\", \"game sessions\", 0, startTutorial1Count, False, True )\n",
    "\n",
    "print('\\nfullMaxCheckpointCountsTable=\\n{0}'.format(fullMaxCheckpointCountsTable))\n",
    "fullMaxCheckpointCountsTable.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration of playing sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "durations = players.groupby(\"sessionId\").agg({ \"serverTime\": [ np.min, np.max  ] })\n",
    "durations[\"duration\"] = pd.to_datetime(durations[\"serverTime\"][\"amax\"]) - pd.to_datetime(durations[\"serverTime\"][\"amin\"])\n",
    "durations[\"duration\"] = durations[\"duration\"].map(lambda x: np.timedelta64(x, 's'))\n",
    "durations = durations.sort_values(by=['duration'], ascending=[False])\n",
    "durations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#durations.loc[:,'duration']\n",
    "#durations = durations[4:]\n",
    "durations[\"duration_seconds\"] = durations[\"duration\"].map(lambda x: pd.Timedelta(x).seconds)\n",
    "maxDuration = np.max(durations[\"duration_seconds\"])\n",
    "durations[\"duration_rank\"] = durations[\"duration_seconds\"].rank(ascending=False)\n",
    "ax = durations.plot(x=\"duration_rank\", y=\"duration_seconds\")\n",
    "plt.xlabel(\"game session\")\n",
    "plt.ylabel(\"time played (s)\")\n",
    "#plt.legend('')\n",
    "ax.legend_.remove()\n",
    "plt.xlim(0, sessionscount)\n",
    "plt.ylim(0, maxDuration)\n",
    "durations[\"duration_seconds\"].describe()\n",
    "#durations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 vs Phase 2 comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCompletedRate(rmdfPlaytestTotalPretestPosttestUniqueProfilesVolunteers),\\\n",
    "getCompletedRate(rmdfPlaytestPhase1PretestPosttestUniqueProfilesVolunteers),\\\n",
    "getCompletedRate(rmdfPlaytestPhase2PretestPosttestUniqueProfilesVolunteers),\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmdfPlaytestPhase1PretestPosttestUniqueProfilesVolunteers[QUserId].nunique(),\\\n",
    "rmdfPlaytestPhase2PretestPosttestUniqueProfilesVolunteers[QUserId].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCompletedRate(rmdfWebgl1522Timed),\\\n",
    "getCompletedRate(rmdfWebgl160Timed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scores\n",
    "\n",
    "scoresPhase1 = allDataPlaytestPhase1PretestPosttestUniqueProfiles.loc['scoreposttest',:]\n",
    "scoresPhase2 = allDataPlaytestPhase2PretestPosttestUniqueProfiles.loc['scoreposttest',:]\n",
    "\n",
    "ttest = ttest_ind(scoresPhase1, scoresPhase2)\n",
    "ttest\n",
    "print(\"t test: statistic=\" + repr(ttest.statistic) + \" pvalue=\" + repr(ttest.pvalue))\n",
    "\n",
    "scoresPhase1 = allDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.loc['scoreposttest',:]\n",
    "scoresPhase2 = allDataPlaytestPhase2PretestPosttestUniqueProfilesVolunteers.loc['scoreposttest',:]\n",
    "\n",
    "ttest = ttest_ind(scoresPhase1, scoresPhase2)\n",
    "ttest\n",
    "print(\"t test: statistic=\" + repr(ttest.statistic) + \" pvalue=\" + repr(ttest.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs = [\"{0:0=2d}\".format(i) for i in range(0,15)]\n",
    "completions = ['ch' + nb + 'completion' for nb in nbs]\n",
    "totals = ['ch' + nb + 'total' for nb in nbs]\n",
    "timeLabels = ['totalTime', 'completionTime'] + completions + totals\n",
    "for timeLabel in timeLabels:\n",
    "    timesPhase1 = allDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.loc[timeLabel,:]\n",
    "    timesPhase2 = allDataPlaytestPhase2PretestPosttestUniqueProfilesVolunteers.loc[timeLabel,:]\n",
    "\n",
    "    ttest = ttest_ind(timesPhase1, timesPhase2)\n",
    "    ttest\n",
    "    print(timeLabel + \" t test: statistic=\" + repr(ttest.statistic) + \" pvalue=\" + repr(ttest.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataPlaytestPhase1PretestPosttestUniqueProfilesVolunteers.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Played time on critical checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRecordPlayer(rmdf1522, gform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRecordPlayer(rmdf160, gform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
