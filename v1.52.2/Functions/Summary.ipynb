{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero.Coli Data Analysis Summary\n",
    "\n",
    "Interactive list of readworthy results from Hero.Coli data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Preparation](#preparation)\n",
    "1. [Google form analysis](#gfdf)\n",
    "2. [Game sessions](#sessions)\n",
    "3. [Per session and per user analysis](#peruser)\n",
    "4. [User comparison](#usercomp)\n",
    "5. [Game map](#map)\n",
    "    1. [List of questions](#qlist)\n",
    "    2. [English](#enform)\n",
    "    3. [French](#frform)\n",
    "    4. [Language selection](#langsel)\n",
    "3. [Basic operations](#basicops)\n",
    "4. [Checkpoint / Question matching](#checkquestmatch)\n",
    "5. [Correlations between durations and score on questions](#Correlations-between-durations-and-score-on-questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "<a id=preparation />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run \"../Functions/6. Time analysis.ipynb\"\n",
    "%run \"../Functions/Plot.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getUserCraftEventsTotal('remove', rmdf.iloc[0]['userId'], rmdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the sample here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online 1.52.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gfdf = gfdfWebgl1522PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#rmdf = rmdfWebgl1522PretestPosttestUniqueProfilesVolunteers.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gfdf = gfdfPlaytestPretestPosttestUniqueProfilesVolunteers.copy()\n",
    "gfdf = gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1.copy()\n",
    "#gfdf = gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase2.copy()\n",
    "\n",
    "#rmdf = rmdfPlaytestPretestPosttestUniqueProfilesVolunteers.copy()\n",
    "rmdf = rmdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1.copy()\n",
    "#rmdf = rmdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online 1.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gfdf = gfdfWebgl160PretestPosttestUniqueProfilesVolunteers.copy()\n",
    "#rmdf = rmdfWebgl160PretestPosttestUniqueProfilesVolunteers.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# small sample\n",
    "#allData = getAllUserVectorData( getAllUsers( rmdf )[:10] )\n",
    "\n",
    "# complete set\n",
    "#allData = getAllUserVectorData( getAllUsers( rmdf ) )\n",
    "\n",
    "# subjects who answered the gfdf\n",
    "allData = getAllUserVectorData( getAllResponders(gfdf), _rmDF = rmdf, _gfDF = gfdf, _source = correctAnswers + demographicAnswers )\n",
    "\n",
    "# 10 subjects who answered the gfdf\n",
    "#allData = getAllUserVectorData( getAllResponders(gfdf)[:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allDataPlaytestPretestPosttestUniqueProfilesPhase1 = getAllUserVectorData(\n",
    "    getAllResponders(gfdfPlaytestPretestPosttestUniqueProfilesPhase1),\n",
    "    _rmDF = rmdfPlaytestPretestPosttestUniqueProfilesPhase1,\n",
    "    _gfDF = gfdfPlaytestPretestPosttestUniqueProfilesPhase1,\n",
    "    _source = correctAnswers + demographicAnswers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allDataPlaytestPretestPosttestUniqueProfilesVolunteersPhase1 = getAllUserVectorData(\n",
    "    getAllResponders(gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1),\n",
    "    _rmDF = rmdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "    _gfDF = gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "    _source = correctAnswers + demographicAnswers )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Google form analysis\n",
    "<a id=gfdf />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample:               gform\")\n",
    "print(\"surveys:              %s\" % len(gform))\n",
    "print(\"unique users:         %s\" % getUniqueUserCount(gform))\n",
    "print(\"RM before:            %s\" % len(gform[gform[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before:            %s\" % len(getGFormBefores(gform)))\n",
    "print(\"RM after:             %s\" % len(gform[gform[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after:             %s\" % len(getGFormAfters(gform)))\n",
    "print(\"unique biologists:    %s\" % getUniqueUserCount(getSurveysOfBiologists(gform)))\n",
    "print(\"unique gamers:        %s\" % getUniqueUserCount(getSurveysOfGamers(gform)))\n",
    "print(\"unique perfect users: %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gform)))\n",
    "print(\"unique perfect users: %s\" % getPerfectPretestPostestPairsCount(gform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample:               gfdf\")\n",
    "print(\"surveys:              %s\" % len(gfdf))\n",
    "print(\"unique users:         %s\" % getUniqueUserCount(gfdf))\n",
    "print(\"RM before:            %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before:            %s\" % len(getGFormBefores(gfdf)))\n",
    "print(\"RM after:             %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after:             %s\" % len(getGFormAfters(gfdf)))\n",
    "print(\"unique biologists:    %s\" % getUniqueUserCount(getSurveysOfBiologists(gfdf)))\n",
    "print(\"unique gamers:        %s\" % getUniqueUserCount(getSurveysOfGamers(gfdf)))\n",
    "print(\"unique perfect users: %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gfdf)))\n",
    "print(\"unique perfect users: %s\" % getPerfectPretestPostestPairsCount(gfdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### formatted version for nice display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"category | count\")\n",
    "print(\"--- | ---\")\n",
    "print(\"sample | gform\")\n",
    "print(\"surveys | %s\" % len(gform))\n",
    "print(\"unique users | %s\" % getUniqueUserCount(gform))\n",
    "print(\"RM before | %s\" % len(gform[gform[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before | %s\" % len(getGFormBefores(gform)))\n",
    "print(\"RM after | %s\" % len(gform[gform[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after | %s\" % len(getGFormAfters(gform)))\n",
    "print(\"unique biologists | %s\" % getUniqueUserCount(getSurveysOfBiologists(gform)))\n",
    "print(\"unique gamers | %s\" % getUniqueUserCount(getSurveysOfGamers(gform)))\n",
    "print(\"unique perfect users | %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gform)))\n",
    "print(\"unique perfect users | %s\" % getPerfectPretestPostestPairsCount(gform))\n",
    "print()\n",
    "#print(\"(\" + str(pd.to_datetime('today').date()) + \")\")\n",
    "print(\"(\"+dataFilesNamesStem+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"category | count\")\n",
    "print(\"--- | ---\")\n",
    "print(\"sample | gfdf\")\n",
    "print(\"surveys | %s\" % len(gfdf))\n",
    "print(\"unique users | %s\" % getUniqueUserCount(gfdf))\n",
    "print(\"RM before | %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[0]]))\n",
    "print(\"GF before | %s\" % len(getGFormBefores(gfdf)))\n",
    "print(\"RM after | %s\" % len(gfdf[gfdf[QTemporality] == answerTemporalities[1]]))\n",
    "print(\"GF after | %s\" % len(getGFormAfters(gfdf)))\n",
    "print(\"unique biologists | %s\" % getUniqueUserCount(getSurveysOfBiologists(gfdf)))\n",
    "print(\"unique gamers | %s\" % getUniqueUserCount(getSurveysOfGamers(gfdf)))\n",
    "print(\"unique perfect users | %s\" % getUniqueUserCount(getSurveysOfUsersWhoAnsweredBoth(gfdf)))\n",
    "print(\"unique perfect users | %s\" % getPerfectPretestPostestPairsCount(gfdf))\n",
    "print()\n",
    "#print(\"(\" + str(pd.to_datetime('today').date()) + \")\")\n",
    "print(\"(\"+dataFilesNamesStem+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 complete sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(gfdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Per temporality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 answered only before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_befores = getGFormBefores(gfdf)\n",
    "rm_befores = getRMBefores(gfdf)\n",
    "gfrm_befores = getRMBefores(getGFormBefores(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gf_befores[QUserId] == rm_befores[QUserId]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gf_befores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 answered only after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_afters = getGFormAfters(gfdf)\n",
    "rm_afters = getRMAfters(gfdf)\n",
    "gfrm_afters = getRMAfters(getGFormBefores(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gf_afters[QUserId] == rm_afters[QUserId]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gf_afters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 answered both before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gf_both = getSurveysOfUsersWhoAnsweredBoth(gfdf, gfMode = True, rmMode = False)\n",
    "rm_both = getSurveysOfUsersWhoAnsweredBoth(gfdf, gfMode = False, rmMode = True)\n",
    "gfrm_both = getSurveysOfUsersWhoAnsweredBoth(gfdf, gfMode = True, rmMode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gf_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(rm_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getDemographicSamples(gfrm_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 pretest vs posttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.4.1 phase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrixToDisplay = plotBasicStats(gfdf, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrixToDisplay.to_csv(\"../../data/sortedPrePostProgression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrixToDisplay.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Per demography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 English speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortEN = gfdf[gfdf[QLanguage] == enLanguageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 French speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortFR = gfdf[gfdf[QLanguage] == frLanguageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortFR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortF = gfdf[gfdf[QGender] == 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortM = gfdf[gfdf[QGender] == 'Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 biologists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortBioS = getSurveysOfBiologists(gfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortBioS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortBioB = getSurveysOfBiologists(gfdf, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortBioB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.6 gamers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohortGamS = getSurveysOfGamers(gfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortGamS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohortGamB = getSurveysOfGamers(gfdf, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotSamples(getTemporalitySamples(cohortGamB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 answered only after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 answers to scientific questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciBinarizedBefore = getAllBinarized(getRMBefores(gfdf))\n",
    "#sciBinarizedBefore = getAllBinarized(getGFBefores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        sciBinarizedBefore,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlations on survey questions before',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        sciBinarizedBefore,\n",
    "                        _abs=True,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciBinarizedAfter = getAllBinarized(getRMAfters(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        sciBinarizedAfter,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlations on survey questions after',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        sciBinarizedAfter,\n",
    "                        _abs=False,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisClustermap.ax_heatmap.annotate(overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir(thisClustermap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir(thisClustermap.ax_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vars(thisClustermap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vars(thisClustermap.ax_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 answers to all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allQuestions = correctAnswers + demographicAnswers\n",
    "\n",
    "allBinarized = getAllBinarized(gfdf, _source = allQuestions)\n",
    "allBinarizedBefore = getAllBinarized(getRMBefores(gfdf), _source = allQuestions)\n",
    "allBinarizedAfter = getAllBinarized(getRMAfters(gfdf), _source = allQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        allBinarized,\n",
    "                        _abs=True,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlation of all answers',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        allBinarizedAfter,\n",
    "                        _abs=True,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 answers to all questions, only before having played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotCorrelationMatrix( _binarizedMatrix, _title='Questions\\' Correlations', _abs=False, _clustered=False, _questionNumbers=False ):\n",
    "plotCorrelationMatrix(\n",
    "                        allBinarizedBefore,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlations on all questions before',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisClustermap, overlay = plotCorrelationMatrix(\n",
    "                        allBinarizedBefore,\n",
    "                        _abs=True,\n",
    "                        _clustered=True,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _metric='correlation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 answers to all questions, only after having played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(\n",
    "                        allBinarizedAfter,\n",
    "                        _abs=False,\n",
    "                        _clustered=False,\n",
    "                        _questionNumbers=True,\n",
    "                        _annot = True,\n",
    "                        _figsize = (20,20),\n",
    "                        _title='Correlation of all answers after',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Game sessions\n",
    "<a id=sessions />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#startDate = minimum152Date\n",
    "#endDate = maximum152Date\n",
    "\n",
    "startDate = rmdf['userTime'].min().date() - datetime.timedelta(days=1)\n",
    "endDate = rmdf['userTime'].max().date() + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = rmdf['userTime'].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='RedMetrics events', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = rmdf[rmdf['type'] == 'start']['userTime'].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='sessions', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = rmdf.groupby('userId').agg({ \"userTime\": np.min })['userTime'].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='game users', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerDay = gfdf.groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "plotPerDay(valuesPerDay, title='survey answers', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valuesPerDay[pd.to_datetime('2017-09-01', utc=True).date():pd.to_datetime('2017-09-30', utc=True).date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforesPerDay = gfdf[gfdf[QTemporality] == answerTemporalities[0]].groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "aftersPerDay = gfdf[gfdf[QTemporality] == answerTemporalities[1]].groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "undefinedPerDay = gfdf[gfdf[QTemporality] == answerTemporalities[2]].groupby(localplayerguidkey).agg({ QTimestamp: np.min })[QTimestamp].map(lambda t: t.date()).value_counts().sort_index()\n",
    "\n",
    "plotPerDay(beforesPerDay, title='survey befores', startDate=startDate, endDate=endDate)\n",
    "plotPerDay(aftersPerDay, title='survey afters', startDate=startDate, endDate=endDate)\n",
    "plotPerDay(undefinedPerDay, title='survey undefined', startDate=startDate, endDate=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Per session and per user analysis\n",
    "<a id=peruser />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. User comparison\n",
    "<a id=usercomp />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do: transfer part of 1.3's \"'Google form analysis' functions tinkering\" code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## percentagesCrossCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretests = gform[gform[QTemporality] == answerTemporalities[0]]\n",
    "#pretests[pretests[QBBFunctionPlasmid] == ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized = sciBinarizedBefore\n",
    "intermediaryNumerator = getCrossCorrectAnswers(binarized).round().astype(int)*100\n",
    "percentagesCrossCorrect = (intermediaryNumerator / binarized.shape[0]).round().astype(int)\n",
    "totalPerQuestion = np.dot(np.ones(binarized.shape[0]), binarized)\n",
    "sciBinarizedBefore.columns[totalPerQuestion == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPercentageCrossCorrect(binarized, figsize=(40,100)):\n",
    "    \n",
    "    cbar_kws = dict(orientation= \"horizontal\")\n",
    "    #cbar_kws = dict(orientation= \"horizontal\",location=\"top\")\n",
    "    #cbar_kws = dict(orientation= \"horizontal\", position=\"top\")\n",
    "    \n",
    "    intermediaryNumerator = getCrossCorrectAnswers(binarized).round().astype(int)*100\n",
    "    percentagesCrossCorrect = (intermediaryNumerator / binarized.shape[0]).round().astype(int)\n",
    "    _fig = plt.figure(figsize=figsize)\n",
    "    _ax = plt.subplot(121)\n",
    "    _ax.set_title('percentage correct')\n",
    "    sns.heatmap(\n",
    "        percentagesCrossCorrect,\n",
    "        ax=_ax,\n",
    "        cmap=plt.cm.jet,\n",
    "        square=True,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cbar_kws=cbar_kws,\n",
    "        vmin=0,\n",
    "        vmax=100,\n",
    "    )\n",
    "    \n",
    "    totalPerQuestion = np.dot(np.ones(binarized.shape[0]), binarized)\n",
    "    totalPerQuestion[totalPerQuestion == 0] = 1\n",
    "    percentagesConditionalCrossCorrect = (intermediaryNumerator / totalPerQuestion).round().astype(int).fillna(0)\n",
    "    _ax = plt.subplot(122)\n",
    "    _ax.set_title('percentage correct, conditionnally: p(y | x)')\n",
    "    sns.heatmap(\n",
    "        percentagesConditionalCrossCorrect,\n",
    "        ax=_ax,\n",
    "        cmap=plt.cm.jet,\n",
    "        square=True,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cbar_kws=cbar_kws,\n",
    "        vmin=0,\n",
    "        vmax=100,\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getPercentageCrossCorrect(sciBinarizedBefore, figsize=(40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPercentageCrossCorrect(sciBinarizedAfter, figsize=(40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gfdf), len(getAllResponders(gfdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrixToDisplay = plotBasicStats(gfdf, horizontalPlot=True, sortedAlong=\"progression\", figsize=(20,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectCount = allData.shape[1]\n",
    "measuredPretest = 100*allData.loc[pretestScientificQuestions,:].sum(axis='columns')/subjectCount\n",
    "measuredPretest.index = scientificQuestions\n",
    "measuredPosttest = 100*allData.loc[posttestScientificQuestions,:].sum(axis='columns')/subjectCount\n",
    "measuredPosttest.index = scientificQuestions\n",
    "measuredDelta2 = (measuredPosttest - measuredPretest)\n",
    "measuredDelta2 = pd.DataFrame(measuredDelta2.round().astype(int))\n",
    "measuredDelta2.columns = [\"measuredDelta2\"]\n",
    "measuredDelta2 = measuredDelta2.sort_values(by = \"measuredDelta2\", ascending = True).T\n",
    "_fig = plt.figure(figsize=(20,2))\n",
    "_ax1 = plt.subplot(111)\n",
    "_ax1.set_title(\"measuredDelta2\")\n",
    "sns.heatmap(\n",
    "            measuredDelta2,\n",
    "            ax=_ax1,\n",
    "            cmap=plt.cm.jet,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            vmin=0,\n",
    "            vmax=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(matrixToDisplay.loc['progression',scientificQuestions] - measuredDelta2.loc['measuredDelta2',scientificQuestions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.DataFrame(columns=[\n",
    "    'pretest1', 'posttest1', 'measuredDelta',\n",
    "    'pretest2', 'posttest2', 'matrixToDisplay'], data = 0, index= scientificQuestions)\n",
    "testDF['pretest1'] = measuredPretest\n",
    "testDF['posttest1'] = measuredPosttest\n",
    "testDF['measuredDelta'] = measuredDelta2.T['measuredDelta2']\n",
    "testDF['pretest2'] = matrixToDisplay.T['pretest'][scientificQuestions]\n",
    "testDF['posttest2'] = matrixToDisplay.T['posttest'][scientificQuestions]\n",
    "testDF['matrixToDisplay'] = matrixToDisplay.T['progression'][scientificQuestions]\n",
    "testDF = testDF.round().astype(int)\n",
    "#testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuredDelta = allData.loc[deltaScientificQuestions,:].sum(axis='columns')\n",
    "measuredDelta.mean(), measuredDelta.median()\n",
    "#measuredDelta.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pretestData = getAllUserVectorData( gfdf[gfdf[QTemporality] == answerTemporalities[0]], _source = correctAnswers )\n",
    "#posttestData = getAllUserVectorData( gfdf[gfdf[QTemporality] == answerTemporalities[1]], _source = correctAnswers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAllUserVectorDataCorrelationMatrix(\n",
    "    allData.T,\n",
    "    _abs=False,\n",
    "    _figsize = (40,40),\n",
    "    _clustered=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demographicCriteria = demographicQuestions.copy()\n",
    "\n",
    "plotAllUserVectorDataCorrelationMatrix(\n",
    "    allData.T,\n",
    "    _abs=False,\n",
    "    _figsize = (20,20),\n",
    "    _clustered=False,\n",
    "    columnSubset=[]\\\n",
    "        + completionTimesCriteria\n",
    "        + totalTimesCriteria\n",
    "        + pretestScientificQuestions\n",
    "        #+ posttestScientificQuestions\n",
    "        #+ deltaScientificQuestions\n",
    "        + overallScoreCriteria\n",
    "        #+ demographicCriteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completers = rmdf[rmdf['type'] == 'complete'][QUserId]\n",
    "#nonCompleter = rmdf[~rmdf[QUserId].isin(completers)][QUserId].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getUserDataVector(nonCompleter)#.loc[14,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allData.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# completed vs played time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(index=allData.columns, columns=[\"time\", \"posttestScore\", \"deltaScore\",\"completed\"])\n",
    "for userId in data.index:\n",
    "    data.loc[userId, \"time\"] = getPlayedTimeUser(userId, _rmDF = rmdf)['tutorial']['totalSpentTime'].total_seconds()\n",
    "    data.loc[userId, \"posttestScore\"] = allData.loc['scoreposttest', userId]\n",
    "    data.loc[userId, \"pretestScore\"] = allData.loc['scorepretest', userId]\n",
    "    data.loc[userId, \"deltaScore\"] = allData.loc['scoredelta', userId]\n",
    "    data.loc[userId, \"completed\"] = allData.loc['complete', userId]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = allScores.copy()\n",
    "x2 = completedScores.copy()\n",
    "y = allPlayedTimes.copy()\n",
    "y2 = completedPlayedTimes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotDF = pd.DataFrame(index = x.index, data = x)\n",
    "plotDF['times'] = y\n",
    "#plotDF\n",
    "#(plotDF['times'] == y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"posttestScore\"]\n",
    "x2 = data[data[\"completed\"]==1][\"posttestScore\"]\n",
    "y = data[\"time\"]\n",
    "y2 = data[data[\"completed\"]==1][\"time\"]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "ax1 = plt.subplot(121)\n",
    "plt.scatter(x, y)#, c='blue', alpha=0.5)\n",
    "plt.scatter(x2, y2)#, c='red', alpha=0.5)\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('time')\n",
    "plt.title(\"time against score, n=\" + str(len(x)))\n",
    "#ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.scatter(y, x)\n",
    "plt.scatter(y2, x2)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('score')\n",
    "plt.title(\"score against time, n=\" + str(len(x)))\n",
    "ax2.legend(loc='center left', bbox_to_anchor=(-1.2, 0.9), labels =[\"unfinished games\",\"completed games\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"posttestScore\"].astype(float)\n",
    "x2 = data[data[\"completed\"]==1][\"posttestScore\"].astype(float)\n",
    "y = data[\"time\"].astype(float)\n",
    "y2 = data[data[\"completed\"]==1][\"time\"].astype(float)\n",
    "\n",
    "# Get the linear models\n",
    "lm_original = np.polyfit(x, y, 1)\n",
    " \n",
    "# calculate the y values based on the co-efficients from the model\n",
    "r_x, r_y = zip(*((i, i*lm_original[0] + lm_original[1]) for i in x))\n",
    " \n",
    "# Put in to a data frame, to keep is all nice\n",
    "lm_original_plot = pd.DataFrame({\n",
    "'scores' : r_x,\n",
    "'times' : r_y\n",
    "})\n",
    "\n",
    "lm_original_plot = lm_original_plot.drop_duplicates()\n",
    "lm_original_plot = lm_original_plot.sort_values(by=\"scores\")\n",
    "lm_original_plot = lm_original_plot.drop(lm_original_plot.index[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot(111)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x2, y2)\n",
    "# Plot the original data and model\n",
    "#lm_original_plot.plot(kind='line', color='Red', x='scores', y='times', ax=ax)\n",
    "plt.plot('scores', 'times', data=lm_original_plot, color='Red')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('time') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x = data[\"posttestScore\"].astype(float)\n",
    "x2 = data[data[\"completed\"]==1][\"posttestScore\"].astype(float)\n",
    "y = data[\"time\"].astype(float)\n",
    "y2 = data[data[\"completed\"]==1][\"time\"].astype(float)\n",
    "\n",
    "xReshaped = x.values.reshape(-1, 1)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(xReshaped, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred = regr.predict(xReshaped)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y, pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(x, y, color='black')\n",
    "plt.plot(x, pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.intercept_,regr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, color=\"b\")\n",
    "plt.scatter(x2, y2, color='red')\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"time played\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.DataFrame(index = range(0, len(xReshaped)), data = xReshaped, columns = ['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['time'] = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data2 = data.loc[:, [\"time\", \"posttestScore\"]]\n",
    "data2.index = range(0, data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import patsy\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "\n",
    "timeScoreformula = 'time ~ posttestScore'\n",
    "\n",
    "# create a fitted model\n",
    "lm1 = smf.ols(formula=timeScoreformula, data=data2).fit()\n",
    "\n",
    "# print the coefficients\n",
    "#lm1.params\n",
    "\n",
    "\n",
    "#lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confidence intervals for the model coefficients\n",
    "lm1.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the p-values for the model coefficients\n",
    "# Represents the probability that the coefficient is actually zero\n",
    "lm1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the R-squared value for the model\n",
    "lm1.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed vs non-completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATSMODELS ###\n",
    "timeScoreformula = 'time ~ posttestScore'\n",
    "lm1 = smf.ols(formula=timeScoreformula, data=data2).fit()\n",
    "lm2 = smf.ols(formula=timeScoreformula, data=data2[data2[\"completed\"] == 0]).fit()\n",
    "lm3 = smf.ols(formula=timeScoreformula, data=data2[data2[\"completed\"] == 1]).fit()\n",
    "lm1.rsquared,lm2.rsquared,lm3.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['deltaScoreRate'] = data['deltaScore']/data['pretestScore']\n",
    "meanDelta = data['deltaScore'].mean()\n",
    "meanPretest = data['pretestScore'].mean()\n",
    "meanDelta/meanPretest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations between durations and score on questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlations between completion time of checkpoint n and score on question Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallScoreCriteria = [\"scorepretest\", \"scoreposttest\", \"scoredelta\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemTimesCriteria = [\"ch\" + \"{0:0=2d}\".format(i) for i in range(0,15)]\n",
    "completionTimesCriteria = [st + \"completion\" for st in stemTimesCriteria] + [\"completionTime\"]\n",
    "totalTimesCriteria = [st + \"total\" for st in stemTimesCriteria] + [\"totalTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#chosenPrefix = answerTemporalities[0]\n",
    "chosenPrefix = answerTemporalities[1]\n",
    "#chosenPrefix = \"delta\"\n",
    "\n",
    "chosenCriteria = [chosenPrefix + \" \" + q for q in scientificQuestions]\n",
    "\n",
    "durationsScoresCorrelations = pd.DataFrame(index=completionTimesCriteria+totalTimesCriteria, columns=chosenCriteria, data=np.nan)\n",
    "durationsScoresCorrelations = durationsScoresCorrelations.rename(str, axis='rows')\n",
    "annotationMatrix = np.empty(shape=[durationsScoresCorrelations.shape[0], 1], dtype=int)\n",
    "#annotationMatrix2D = np.empty(durationsScoresCorrelations.shape, dtype=str)\n",
    "\n",
    "allData2 = allData.T.rename(str,axis=\"columns\")\n",
    "for i in range(len(durationsScoresCorrelations.index)):\n",
    "    checkpoint = durationsScoresCorrelations.index[i]\n",
    "    allData3 = allData2[allData2[checkpoint] < pd.Timedelta.max.total_seconds()]\n",
    "    annotationMatrix[i] = len(allData3)\n",
    "    for q in durationsScoresCorrelations.columns:\n",
    "        corr = np.corrcoef(allData3[checkpoint], allData3[q])\n",
    "        if corr[0,0] < 0:\n",
    "            print(\"[\" + checkpoint + \";\" + q + \"]:\" + str(corr[0,0]))\n",
    "        #if pd.isnull(corr[0,1]):\n",
    "        #    print(\"[\" + checkpoint + \";\" + q + \"] null\")\n",
    "        durationsScoresCorrelations.loc[checkpoint, q] = corr[0,1]\n",
    "        \n",
    "_fig, (_a0, _a1) = plt.subplots(1,2, gridspec_kw = {'width_ratios':[50, 1]}, figsize=(15,10))\n",
    "\n",
    "_a0.set_title(\"correlations between times and \" + chosenPrefix + \" scores\")\n",
    "sns.heatmap(durationsScoresCorrelations, ax=_a0, cmap=plt.cm.jet, square=True, vmin=-1, vmax=1,)\n",
    "            # annot=annotationMatrix2D\n",
    "            #cbar_kws= {'panchor':(0.0, 0.0)}\n",
    "\n",
    "_a1.set_title(\"\")\n",
    "sns.heatmap(annotationMatrix, ax=_a1, annot=annotationMatrix)\n",
    "\n",
    "_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUserId = gfdf[QUserId].unique()[12]\n",
    "getCheckpointsTotalTimesUser(testUserId, rmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timedSectionnedEvents.to_csv(\"ch4.csv\", encoding=csvEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getAllResponders(gfdf), _source = correctAnswers, _rmDF = rmdf\n",
    "#testUserId = \"4731525f-62dd-4128-ab56-3991b403e17e\"\n",
    "#getUserDataVector(testUserId,_source = correctAnswers, _rmDF = rmdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Game map\n",
    "<a id=map />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#players = rmdf.loc[:, playerFilteringColumns]\n",
    "players = safeGetNormalizedRedMetricsCSV( rmdf )\n",
    "players.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#players = players.dropna(how='any')\n",
    "#players.head(1)\n",
    "#rmdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#players = players[~players['userId'].isin(excludedIDs)];\n",
    "#players.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions (filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionscount = players[\"sessionId\"].nunique()\n",
    "sessionscount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions of dev IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueplayers = players['userId']\n",
    "uniqueplayers = uniqueplayers.unique()\n",
    "uniqueplayers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#uniqueplayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueplatforms = players['customData.platform'].unique()\n",
    "uniqueplatforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints passed / furthest checkpoint (unfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = rmdf.loc[:, ['type', 'section', 'sessionId']]\n",
    "checkpoints = checkpoints[checkpoints['type']=='reach'].loc[:,['section','sessionId']]\n",
    "checkpoints = checkpoints[checkpoints['section'].str.startswith('tutorial', na=False)]\n",
    "checkpoints = checkpoints.groupby(\"sessionId\")\n",
    "checkpoints = checkpoints.max()\n",
    "#len(checkpoints)\n",
    "checkpoints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxCheckpointTable = pd.DataFrame({\"maxCheckpoint\" : checkpoints.values.flatten()})\n",
    "maxCheckpointCounts = maxCheckpointTable[\"maxCheckpoint\"].value_counts()\n",
    "maxCheckpointCounts['Start'] = None\n",
    "maxCheckpointCounts = maxCheckpointCounts.sort_index()\n",
    "print('\\nmaxCheckpointCounts=\\n{0}'.format(str(maxCheckpointCounts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxCheckpointCountsTable = pd.DataFrame({\"maxCheckpoint\" : maxCheckpointCounts.values})\n",
    "maxCheckpointCountsTableCount = maxCheckpointCountsTable.sum(0)[0]\n",
    "maxCheckpointCountsTableCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoints.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCheckpointCountsTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCheckpointCountsTable.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genericTreatment( maxCheckpointCountsTable, \"best checkpoint reached\", \"game sessions\", 0, maxCheckpointCountsTableCount, False, True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts = rmdf.loc[:, checkpointsRelevantColumns]\n",
    "#starts = checkpoints[checkpoints['type']=='start'].loc[:,['playerId']]\n",
    "#starts = checkpoints[checkpoints['section'].str.startswith('tutorial', na=False)]\n",
    "#starts = checkpoints.groupby(\"playerId\")\n",
    "#starts = checkpoints.max()\n",
    "#starts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTutorial1Count = sessionscount\n",
    "neverReachedGameSessionCount = startTutorial1Count - maxCheckpointCountsTableCount\n",
    "fullMaxCheckpointCounts = maxCheckpointCounts\n",
    "fullMaxCheckpointCounts['Start'] = neverReachedGameSessionCount\n",
    "fullMaxCheckpointCountsTable = pd.DataFrame({\"fullMaxCheckpoint\" : fullMaxCheckpointCounts.values})\n",
    "\n",
    "genericTreatment( fullMaxCheckpointCountsTable, \"best checkpoint reached\", \"game sessions\", 0, startTutorial1Count, False, True )\n",
    "\n",
    "print('\\nfullMaxCheckpointCountsTable=\\n{0}'.format(fullMaxCheckpointCountsTable))\n",
    "fullMaxCheckpointCountsTable.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration of playing sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "durations = players.groupby(\"sessionId\").agg({ \"serverTime\": [ np.min, np.max  ] })\n",
    "durations[\"duration\"] = pd.to_datetime(durations[\"serverTime\"][\"amax\"]) - pd.to_datetime(durations[\"serverTime\"][\"amin\"])\n",
    "durations[\"duration\"] = durations[\"duration\"].map(lambda x: np.timedelta64(x, 's'))\n",
    "durations = durations.sort_values(by=['duration'], ascending=[False])\n",
    "durations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#durations.loc[:,'duration']\n",
    "#durations = durations[4:]\n",
    "durations[\"duration_seconds\"] = durations[\"duration\"].map(lambda x: pd.Timedelta(x).seconds)\n",
    "maxDuration = np.max(durations[\"duration_seconds\"])\n",
    "durations[\"duration_rank\"] = durations[\"duration_seconds\"].rank(ascending=False)\n",
    "ax = durations.plot(x=\"duration_rank\", y=\"duration_seconds\")\n",
    "plt.xlabel(\"game session\")\n",
    "plt.ylabel(\"time played (s)\")\n",
    "#plt.legend('')\n",
    "ax.legend_.remove()\n",
    "plt.xlim(0, sessionscount)\n",
    "plt.ylim(0, maxDuration)\n",
    "durations[\"duration_seconds\"].describe()\n",
    "#durations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 vs Phase 2 comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompletedRate(_rmdf):\n",
    "    players = _rmdf[QUserId].nunique()\n",
    "    completers = _rmdf[_rmdf['type'] == 'complete'][QUserId].nunique()\n",
    "    return float(completers)/float(players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCompletedRate(rmdfPlaytestPretestPosttestUniqueProfilesVolunteers),\\\n",
    "getCompletedRate(rmdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1),\\\n",
    "getCompletedRate(rmdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase2),\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCompletedRate(rmdfWebgl1522Timed),\\\n",
    "getCompletedRate(rmdfWebgl160Timed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Played time on critical checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRecordPlayer(rmdf1522, gform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRecordPlayer(rmdf160, gform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per question analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interest variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QInterestBiology,\\\n",
    "QCuriosityBiology,\\\n",
    "QCuriositySyntheticBiology,\\\n",
    "QCuriosityEngineering,\\\n",
    "QCuriosityVideoGames,\\\n",
    "#QEnjoyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curiosityQuestions = [\n",
    "    #QInterestBiology,\\\n",
    "    QCuriosityBiology,\\\n",
    "    QCuriositySyntheticBiology,\\\n",
    "    QCuriosityEngineering,\\\n",
    "    QCuriosityVideoGames,\\\n",
    "    #QEnjoyed,\\\n",
    "]\n",
    "questions = [deltaPrefix + \" \" + q for q in curiosityQuestions]\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseQuestion(allData, q):\n",
    "    pretestScores = allData.loc[answerTemporalities[0] + \" \" + q, :]\n",
    "    posttestScores = allData.loc[answerTemporalities[1] + \" \" + q, :]\n",
    "    deltaScores = allData.loc[deltaPrefix + \" \" + q, :]\n",
    "    print(\"variation: %0.2f (+/- %0.2f)\" % (deltaScores.mean(), deltaScores.std() * 2))\n",
    "    print(\"from %0.2f (+/- %0.2f) to %0.2f (+/- %0.2f)\" % \\\n",
    "          (pretestScores.mean(), pretestScores.std() * 2,\\\n",
    "           posttestScores.mean(), posttestScores.std() * 2,))\n",
    "    plt.boxplot(deltaScores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in curiosityQuestions:\n",
    "    analyseQuestion(allDataPlaytestPretestPosttestUniqueProfilesPhase1, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for q in questions:\n",
    "    varScores = allDataPlaytestPretestPosttestUniqueProfilesPhase1.loc[q, :]\n",
    "    print(q)\n",
    "    print(\"variation: %0.2f (+/- %0.2f)\" % (varScores.mean(), varScores.std() * 2))\n",
    "    plt.boxplot(varScores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varScores = allDataPlaytestPretestPosttestUniqueProfilesPhase1.loc['delta Want to learn more about Synthetic biology', :]\n",
    "print(\"variation: %0.2f (+/- %0.2f)\" % (varScores.mean(), varScores.std() * 2))\n",
    "plt.boxplot(varScores)\n",
    "plt.show()\n",
    "varScores.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5 scale analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataFormating.ipynb\n",
    "QCuriosityCoding = {\"A lot\": 4, \"Beaucoup\": 4, \"Enormément\": 5, \"Énormément\": 5, \"Extremely\": 5, \"Moderately\": 3, \"Moyennement\": 3, \"Slightly\": 2, \"Un peu\": 2, \"I don't know\": 3, \"Je ne sais pas\": 3, \"Not at all\": 1, \"Pas du tout\": 1}\n",
    "QCuriosityBiologyCoding = QCuriosityCoding\n",
    "QCuriositySyntheticBiologyCoding = QCuriosityCoding\n",
    "QCuriosityEngineeringCoding = QCuriosityCoding\n",
    "QCuriosityVideoGamesCoding = QCuriosityCoding\n",
    "#biologyInterestCoding = {\"A lot\": 4, \"Beaucoup\": 4, \"Enormément\": 5, \"Énormément\": 5, \"Extremely\": 5, \"Moderately\": 3, \"Moyennement\": 3, \"Slightly\": 2, \"Un peu\": 2, \"I don't know\": 3, \"Je ne sais pas\": 3, \"Not at all\": 1, \"Pas du tout\": 1}\n",
    "#QEnjoyedCoding = {'Extremely': 4, 'A lot': 3, 'Not at all': 0, 'A bit': 1, 'Moderately': 2, \"No\": 0, \"Not applicable: not played yet\": -1}\n",
    "\n",
    "curiosityQuestionsCoding = [QCuriosityBiologyCoding, QCuriositySyntheticBiologyCoding, QCuriosityEngineeringCoding, QCuriosityVideoGamesCoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "    gfdfPlaytestPretestPosttestUniqueProfilesPhase1,\n",
    "    curiosityQuestions,\n",
    "    curiosityQuestionsCoding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPretestPosttestDeltaGfdf(gfdfResult, curiosityQuestions, \n",
    "                            plotGraphs = False, printData = True, saveFiles = False, suffix = ' 1-5 score variation'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questionsCoding contains points attributed to each answer\n",
    "def compareUsingCustomCorrection(gfdf, questions, questionsCoding):\n",
    "    \n",
    "    minPotentialScore = 0\n",
    "    maxPotentialScore = 0\n",
    "    for gradingDictionary in questionsCoding:\n",
    "        minPotentialScore += min(gradingDictionary.values())\n",
    "        maxPotentialScore += max(gradingDictionary.values())\n",
    "    minPotentialScore, maxPotentialScore \n",
    "    print(\"%s < score < %s\" % (minPotentialScore, maxPotentialScore))\n",
    "    \n",
    "    # split temporalities\n",
    "    gfdfPretest = gfdf[gfdf[QTemporality]==answerTemporalities[0]]\n",
    "    gfdfPretest.index = gfdfPretest[QUserId]\n",
    "    gfdfPostest = gfdf[gfdf[QTemporality]==answerTemporalities[1]]\n",
    "    gfdfPostest.index = gfdfPostest[QUserId]\n",
    "\n",
    "    # only keep relevant questions\n",
    "    gfdfPretest = gfdfPretest.loc[:, questions]\n",
    "    gfdfPostest = gfdfPostest.loc[:, questions]\n",
    "\n",
    "    # code the answers\n",
    "    for (q, c) in zip(questions, questionsCoding):\n",
    "        gfdfPretest[q] = gfdfPretest[q].apply(lambda t: c[t])\n",
    "        gfdfPostest[q] = gfdfPostest[q].apply(lambda t: c[t])\n",
    "\n",
    "    # compute delta\n",
    "    # gfdfDelta = gfdfPostest - gfdfPretest\n",
    "    \n",
    "    gfdfResult = gfdfPostest - gfdfPretest\n",
    "    gfdfResult.columns =  [deltaPrefix + \" \" + q for q in questions]\n",
    "    gfdfResult[[answerTemporalities[0] + \" \" + q for q in questions]] = gfdfPretest\n",
    "    gfdfResult[[answerTemporalities[1] + \" \" + q for q in questions]] = gfdfPostest\n",
    "     \n",
    "    return gfdfResult.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotPretestPosttestDeltaGfdf(allData, questions,\n",
    "                                 plotGraphs = True, printData = True, saveFiles = False,\n",
    "                                 title = \"\", suffix = \"\"):\n",
    "    \n",
    "    variationSuffix = ' - variation'\n",
    "    pretestPosttestSuffix = ' - pretest posttest'\n",
    "            \n",
    "    # sample size\n",
    "    print(\"n = \" + str(len(allData.columns)))\n",
    "    print()\n",
    "    print()\n",
    "    for q in questions:\n",
    "        deltaScores    = allData.loc[deltaPrefix + \" \" +q             ,:]\n",
    "        pretestScores  = allData.loc[answerTemporalities[0] + \" \" + q ,:]\n",
    "        posttestScores = allData.loc[answerTemporalities[1] + \" \" + q ,:]\n",
    "\n",
    "        if printData:\n",
    "            print(q)\n",
    "            print(\"variation: %0.2f (+/- %0.2f)\" % (deltaScores.mean(), deltaScores.std() * 2))\n",
    "            print(\"from %0.2f (+/- %0.2f) to %0.2f (+/- %0.2f)\" % \\\n",
    "                  (pretestScores.mean(),  pretestScores.std() * 2,\\\n",
    "                   posttestScores.mean(), posttestScores.std() * 2,))\n",
    "            print(ttest_ind(pretestScores, posttestScores))\n",
    "        if plotGraphs:\n",
    "            plt.boxplot(deltaScores)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            ax = plt.subplot(111)\n",
    "#            if pd.isnull(deltaScores).any():\n",
    "#                print(\"pd.isnull(deltaScores).any(): \" + str(deltaScores.index[pd.isnull(deltaScores)]))\n",
    "            plt.hist(deltaScores, bins=int(max(deltaScores) - min(deltaScores) + 1), figure = fig)\n",
    "            #sns.distplot(deltaScores, bins = np.arange(min(deltaScores),max(deltaScores)))\n",
    "            if len(title) == 0:\n",
    "                _title = '\"' + q + '\"' + variationSuffix + suffix\n",
    "            else:\n",
    "                _title = title + variationSuffix\n",
    "            plt.title(_title)    \n",
    "            plt.xlabel(\"score variation\")\n",
    "            plt.ylabel(\"count\")\n",
    "            plt.show()\n",
    "            if saveFiles:\n",
    "                fig.savefig(_title.replace('\"', \"\"))\n",
    "\n",
    "        \n",
    "            fig = plt.figure()\n",
    "            ax = plt.subplot(111)\n",
    "            plt.hist(pretestScores, bins=int(max(pretestScores) - min(pretestScores) + 1), label='pretest', alpha=0.5, figure = fig)\n",
    "            plt.hist(posttestScores, bins=int(max(posttestScores) - min(posttestScores) + 1), label='posttest', alpha=0.5, figure = fig)\n",
    "            plt.legend()\n",
    "            \n",
    "            if len(title) == 0:\n",
    "                _title = '\"' + q + '\"' + pretestPosttestSuffix + suffix\n",
    "            else:\n",
    "                _title = title + pretestPosttestSuffix\n",
    "            plt.title(_title)\n",
    "            plt.xlabel(\"score\")\n",
    "            plt.ylabel(\"count\")\n",
    "            plt.show()\n",
    "            if saveFiles:\n",
    "                fig.savefig(_title.replace('\"', \"\"))\n",
    "        \n",
    "        if printData:\n",
    "            print()\n",
    "            print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseQuestionGroup(allData, Qs, plotGraphs = True, printData = True, saveFiles = False, title = \"\"):\n",
    "    pretestQs  = [answerTemporalities[0] + \" \" + q for q in Qs]\n",
    "    posttestQs = [answerTemporalities[1] + \" \" + q for q in Qs]\n",
    "    deltaQs    = [deltaPrefix            + \" \" + q for q in Qs]    \n",
    "\n",
    "    pretestScores  = allData.loc[pretestQs,  :].sum()\n",
    "    posttestScores = allData.loc[posttestQs, :].sum()\n",
    "    deltaScores    = allData.loc[deltaQs,    :].sum()\n",
    "    \n",
    "    questionGroupStem = \"question group\"\n",
    "    pretestColumn  = answerTemporalities[0] + \" \" + questionGroupStem\n",
    "    posttestColumn = answerTemporalities[1] + \" \" + questionGroupStem\n",
    "    deltaColumn    = deltaPrefix            + \" \" + questionGroupStem\n",
    "    \n",
    "    gfdfResult = pd.DataFrame(data = [deltaScores, pretestScores, posttestScores],\\\n",
    "                              columns = deltaScores.index,\\\n",
    "                              index = [deltaColumn, pretestColumn, posttestColumn])\n",
    "    \n",
    "    plotPretestPosttestDeltaGfdf(gfdfResult, [questionGroupStem], \n",
    "                            plotGraphs = plotGraphs, printData = printData, saveFiles = saveFiles, title = title\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2b Specific scientific questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientificQuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyseQuestion(allData, QGenotypePhenotype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overall binary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h2bcQuestions = [\n",
    "    #'What does this device do? RBS:PCONS:FLHDC:TER XXX', #32\n",
    "    QDeviceRbsPconsFlhdcTer,\n",
    "    #'What does this device do? PCONS:RBS:FLHDC:TER', #33\n",
    "    QDevicePconsRbsFlhdcTer,\n",
    "    #'What does this device do? PBAD:RBS:GFP:TER', #34\n",
    "    QDevicePbadRbsGfpTer,\n",
    "    #'What does this device do? PBAD:GFP:RBS:TER XXX', #35\n",
    "    QDevicePbadGfpRbsTer,\n",
    "    #'What does this device do? GFP:RBS:PCONS:TER XXX', #36\n",
    "    QDeviceGfpRbsPconsTer,\n",
    "    #'What does this device do? PCONS:GFP:RBS:TER XXX', #37\n",
    "    QDevicePconsGfpRbsTer,\n",
    "    #'What does this device do? AMPR:RBS:PCONS:TER XXX', #38\n",
    "    QDeviceAmprRbsPconsTer,\n",
    "    #'What does this device do? RBS:PCONS:AMPR:TER XXX', #39\n",
    "    QDeviceRbsPconsAmprTer,\n",
    "]\n",
    "analyseQuestionGroup(allData, h2bcQuestions, title=\"device questions - binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biobrick grammar analysis\n",
    "More lenient approach: only considering grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDeviceQuestionsGrading(correctAnsCost,halfCorAnsCost,dontKnoAnsCost,incorreAnsCost,):\n",
    "\n",
    "    r0 = DeviceAnswersPossibleAnswersEN[0]\n",
    "    r1 = DeviceAnswersPossibleAnswersEN[1]\n",
    "    r2 = DeviceAnswersPossibleAnswersEN[2]\n",
    "    r3 = DeviceAnswersPossibleAnswersEN[3]\n",
    "    r4 = DeviceAnswersPossibleAnswersEN[4]\n",
    "    r5 = DeviceAnswersPossibleAnswersEN[5]\n",
    "\n",
    "    QDeviceRbsPconsFlhdcTerCoding = {r0:correctAnsCost, r1:incorreAnsCost, r2:incorreAnsCost, r3:incorreAnsCost, r4:incorreAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDevicePconsRbsFlhdcTerCoding = {r0:incorreAnsCost, r1:halfCorAnsCost, r2:halfCorAnsCost, r3:correctAnsCost, r4:halfCorAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDevicePbadRbsGfpTerCoding =    {r0:incorreAnsCost, r1:halfCorAnsCost, r2:correctAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDevicePbadGfpRbsTerCoding =    {r0:correctAnsCost, r1:incorreAnsCost, r2:incorreAnsCost, r3:incorreAnsCost, r4:incorreAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDeviceGfpRbsPconsTerCoding =   {r0:correctAnsCost, r1:incorreAnsCost, r2:incorreAnsCost, r3:incorreAnsCost, r4:incorreAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDevicePconsGfpRbsTerCoding =   {r0:correctAnsCost, r1:incorreAnsCost, r2:incorreAnsCost, r3:incorreAnsCost, r4:incorreAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDeviceAmprRbsPconsTerCoding =  {r0:correctAnsCost, r1:incorreAnsCost, r2:incorreAnsCost, r3:incorreAnsCost, r4:incorreAnsCost, r5:dontKnoAnsCost,}\n",
    "    QDeviceRbsPconsAmprTerCoding =  {r0:correctAnsCost, r1:incorreAnsCost, r2:incorreAnsCost, r3:incorreAnsCost, r4:incorreAnsCost, r5:dontKnoAnsCost,}\n",
    "\n",
    "    return [\n",
    "    QDeviceRbsPconsFlhdcTerCoding,\n",
    "    QDevicePconsRbsFlhdcTerCoding,\n",
    "    QDevicePbadRbsGfpTerCoding,\n",
    "    QDevicePbadGfpRbsTerCoding,\n",
    "    QDeviceGfpRbsPconsTerCoding,\n",
    "    QDevicePconsGfpRbsTerCoding,\n",
    "    QDeviceAmprRbsPconsTerCoding,\n",
    "    QDeviceRbsPconsAmprTerCoding,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeviceAnswersPossibleAnswersEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseGrading = getDeviceQuestionsGrading(1,0,0,0)\n",
    "onlyGrammarGrading = getDeviceQuestionsGrading(2,2,1,0)\n",
    "lenientGrading = getDeviceQuestionsGrading(3,2,1,0)\n",
    "# This strict grading policy penalizes errors, and gives more points for \"I don't know\" answers than to errors.\n",
    "strictGrading = getDeviceQuestionsGrading(3,1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bcQuestions,\n",
    "                                            baseGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bcQuestions, title=\"device questions - base\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bcQuestions,\n",
    "                                            onlyGrammarGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bcQuestions, title=\"device questions - grammar\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bcQuestions,\n",
    "                                            lenientGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bcQuestions, title=\"device questions - lenient\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bcQuestions,\n",
    "                                            strictGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bcQuestions, title=\"device questions - misconceptions\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2bdQuestions = [\n",
    "    #'Represents the end of a device... TER', #20\n",
    "    QBBFunctionTER,\n",
    "    #'Represents the ability given... CDS', #22\n",
    "    QBBFunctionGameCDS,\n",
    "    #'Codes a protein... CDS', #24\n",
    "    QBBFunctionBiologyCDS,\n",
    "    #'Controls when the device is active... PR', #28\n",
    "    QBBFunctionPR,\n",
    "    #'Controls the level of expression, and thus how much the ability will be affected... RBS', #29\n",
    "    QBBFunctionRBS,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBioBrickFunctionsQuestionsGrading(correctAnsCost,halfCorAnsCost,dontKnoAnsCost,incorreAnsCost,):\n",
    "\n",
    "    r0 = BioBrickAnswersPossibleAnswersEN[0]\n",
    "    r1 = BioBrickAnswersPossibleAnswersEN[1]\n",
    "    r2 = BioBrickAnswersPossibleAnswersEN[2]\n",
    "    r3 = BioBrickAnswersPossibleAnswersEN[3]\n",
    "    r4 = BioBrickAnswersPossibleAnswersEN[4]\n",
    "    r5 = BioBrickAnswersPossibleAnswersEN[5]\n",
    "    r6 = BioBrickAnswersPossibleAnswersEN[6]\n",
    "    \n",
    "    QBBFunctionTERCoding        = {r0:incorreAnsCost, r1:correctAnsCost, r2:halfCorAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, r5:incorreAnsCost, r6:dontKnoAnsCost,}\n",
    "    QBBFunctionGameCDSCoding    = {r0:incorreAnsCost, r1:halfCorAnsCost, r2:halfCorAnsCost, r3:correctAnsCost, r4:halfCorAnsCost, r5:incorreAnsCost, r6:dontKnoAnsCost,}\n",
    "    QBBFunctionBiologyCDSCoding = {r0:incorreAnsCost, r1:halfCorAnsCost, r2:halfCorAnsCost, r3:correctAnsCost, r4:halfCorAnsCost, r5:incorreAnsCost, r6:dontKnoAnsCost,}\n",
    "    QBBFunctionPRCoding         = {r0:incorreAnsCost, r1:halfCorAnsCost, r2:correctAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, r5:incorreAnsCost, r6:dontKnoAnsCost,}\n",
    "    QBBFunctionRBSCoding        = {r0:incorreAnsCost, r1:halfCorAnsCost, r2:halfCorAnsCost, r3:halfCorAnsCost, r4:correctAnsCost, r5:incorreAnsCost, r6:dontKnoAnsCost,}\n",
    "\n",
    "    return [\n",
    "    QBBFunctionTERCoding,\n",
    "    QBBFunctionGameCDSCoding,\n",
    "    QBBFunctionBiologyCDSCoding,\n",
    "    QBBFunctionPRCoding,\n",
    "    QBBFunctionRBSCoding,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseGrading = getBioBrickFunctionsQuestionsGrading(1,0,0,0)\n",
    "onlBBTypeGrading = getBioBrickFunctionsQuestionsGrading(2,2,1,0) # same grading if a BioBrick is selected as answer\n",
    "lenientGrading = getBioBrickFunctionsQuestionsGrading(3,2,1,0)\n",
    "strictGrading = getBioBrickFunctionsQuestionsGrading(3,1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bdQuestions,\n",
    "                                            baseGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bdQuestions, title=\"device questions - base\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bdQuestions,\n",
    "                                            onlBBTypeGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bdQuestions, title=\"device questions - BioBrick category\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bdQuestions,\n",
    "                                            lenientGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bdQuestions, title=\"device questions - lenient\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2bdQuestions,\n",
    "                                            strictGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2bdQuestions, title=\"device questions - misconceptions\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2beQuestions = [\n",
    "    #'What does this device do? RBS:PCONS:FLHDC:TER XXX', #32\n",
    "    QDeviceRbsPconsFlhdcTer,\n",
    "    #'What does this device do? PCONS:RBS:FLHDC:TER', #33\n",
    "    QDevicePconsRbsFlhdcTer,\n",
    "    #'What does this device do? GFP:RBS:PCONS:TER XXX', #36\n",
    "    QDeviceGfpRbsPconsTer,\n",
    "    #'What does this device do? PCONS:GFP:RBS:TER XXX', #37\n",
    "    QDevicePconsGfpRbsTer,\n",
    "    #'What does this device do? AMPR:RBS:PCONS:TER XXX', #38\n",
    "    QDeviceAmprRbsPconsTer,\n",
    "    #'What does this device do? RBS:PCONS:AMPR:TER XXX', #39\n",
    "    QDeviceRbsPconsAmprTer,\n",
    "    \n",
    "    #'What does this device do? PBAD:RBS:GFP:TER', #34\n",
    "    QDevicePbadRbsGfpTer,\n",
    "    #'What does this device do? PBAD:GFP:RBS:TER XXX', #35\n",
    "    QDevicePbadGfpRbsTer,\n",
    "    #'What does this device do? GFP:RBS:PCONS:TER XXX', #36    \n",
    "    \n",
    "    #'Last question. Next page only contains remarks.Guess: you have crafted a functional device containing an arabinose-induced promoter and an arabinose Coding Sequence (CDS). What will happen?', #42\n",
    "    QDevicePbadRbsAraTer,\n",
    "]\n",
    "\n",
    "h2beStricterQuestions = [\n",
    "    \n",
    "    #'What does this device do? PBAD:RBS:GFP:TER', #34\n",
    "    QDevicePbadRbsGfpTer,\n",
    "    #'What does this device do? PBAD:GFP:RBS:TER XXX', #35\n",
    "    QDevicePbadGfpRbsTer,\n",
    "    #'What does this device do? GFP:RBS:PCONS:TER XXX', #36    \n",
    "    \n",
    "    #'Last question. Next page only contains remarks.Guess: you have crafted a functional device containing an arabinose-induced promoter and an arabinose Coding Sequence (CDS). What will happen?', #42\n",
    "    QDevicePbadRbsAraTer,\n",
    "]\n",
    "\n",
    "h2beStrictestQuestions = [ \n",
    "    \n",
    "    #'Last question. Next page only contains remarks.Guess: you have crafted a functional device containing an arabinose-induced promoter and an arabinose Coding Sequence (CDS). What will happen?', #42\n",
    "    QDevicePbadRbsAraTer,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyseQuestionGroup(allData, h2beQuestions, title=\"induction questions - binary\")\n",
    "analyseQuestionGroup(allData, h2beStricterQuestions, title=\"induction questions - binary\")\n",
    "analyseQuestionGroup(allData, h2beStrictestQuestions, title=\"induction questions - binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDevicePbadRbsAraTerPossibleAnswersEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInductionQuestionsGrading(correctAnsCost,halfCorAnsCost,dontKnoAnsCost,incorreAnsCost,):\n",
    "\n",
    "    r0 = DeviceAnswersPossibleAnswersEN[0]\n",
    "    r1 = DeviceAnswersPossibleAnswersEN[1]\n",
    "    r2 = DeviceAnswersPossibleAnswersEN[2]\n",
    "    r3 = DeviceAnswersPossibleAnswersEN[3]\n",
    "    r4 = DeviceAnswersPossibleAnswersEN[4]\n",
    "\n",
    "    r5 = QDevicePbadRbsAraTerPossibleAnswersEN[0]\n",
    "    r6 = QDevicePbadRbsAraTerPossibleAnswersEN[1]\n",
    "    r7 = QDevicePbadRbsAraTerPossibleAnswersEN[2]\n",
    "    r8 = QDevicePbadRbsAraTerPossibleAnswersEN[3]\n",
    "\n",
    "    rIDK = DeviceAnswersPossibleAnswersEN[5]\n",
    "\n",
    "#                            'The bricks are not well-ordered',\n",
    "#                                                     'It generates green fluorescence',\n",
    "#                                                     'It generates green fluorescence in presence of arabinose inducer',\n",
    "#                                                                                     'It makes it possible to move faster',\n",
    "#                                                                                                         'It generates antibiotic resistance'\n",
    "    QDeviceRbsPconsFlhdcTerGrading = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:incorreAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDevicePconsRbsFlhdcTerGrading = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:incorreAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDeviceGfpRbsPconsTerGrading   = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:incorreAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDevicePconsGfpRbsTerGrading   = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:incorreAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDeviceAmprRbsPconsTerGrading  = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:incorreAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDeviceRbsPconsAmprTerGrading  = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:incorreAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDevicePbadRbsGfpTerGrading    = {r0:halfCorAnsCost, r1:incorreAnsCost, r2:correctAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "    QDevicePbadGfpTbsTerGrading    = {r0:halfCorAnsCost, r1:halfCorAnsCost, r2:halfCorAnsCost, r3:halfCorAnsCost, r4:halfCorAnsCost, rIDK:dontKnoAnsCost,}\n",
    "\n",
    "    QDevicePbadRbsAraTerGrading    = {r5:halfCorAnsCost, r6:halfCorAnsCost, r7:correctAnsCost, r8:incorreAnsCost, rIDK:dontKnoAnsCost,}\n",
    "\n",
    "    return [\n",
    "    QDeviceRbsPconsFlhdcTerGrading,\n",
    "    QDevicePconsRbsFlhdcTerGrading,\n",
    "    QDeviceGfpRbsPconsTerGrading,\n",
    "    QDevicePconsGfpRbsTerGrading,\n",
    "    QDeviceAmprRbsPconsTerGrading,\n",
    "    QDeviceRbsPconsAmprTerGrading,\n",
    "    QDevicePbadRbsGfpTerGrading,\n",
    "    QDevicePbadGfpTbsTerGrading,\n",
    "    QDevicePbadRbsAraTerGrading,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenientGrading = getInductionQuestionsGrading(3,2,1,0)\n",
    "strictGrading = getInductionQuestionsGrading(3,1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2beQuestions,\n",
    "                                            lenientGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2beQuestions, title=\"induction questions - lenientGrading\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gfdfResult = compareUsingCustomCorrection(\n",
    "                                            gfdfPlaytestPretestPosttestUniqueProfilesVolunteersPhase1,\n",
    "                                            h2beQuestions,\n",
    "                                            strictGrading,\n",
    "                                        )\n",
    "analyseQuestionGroup(gfdfResult, h2beQuestions, title=\"induction questions - strictGrading\",\n",
    "                     plotGraphs = False, printData = True, saveFiles = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
